{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "14b6e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wiki\n",
    "from collections import defaultdict, Counter \n",
    "import re\n",
    "import heapq\n",
    "import time\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import urllib\n",
    "\n",
    "from requests_html import HTMLSession\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html_text\n",
    "\n",
    "import nltk\n",
    "from os.path import isfile, join\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from transformers import pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11f94d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes extraneous s from the end of a title\n",
    "def clean_title(title):\n",
    "    return title[:-1].lower() if title[-1] == 's' else title.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fa877a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE = clean_title(\"computer architecture\")\n",
    "SUBSECTIONS = 4\n",
    "RELATED_TITLES = wiki.search(TITLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a22db47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Computer architecture',\n",
       " 'Word (computer architecture)',\n",
       " 'Multithreading (computer architecture)',\n",
       " 'Hazard (computer architecture)',\n",
       " 'Von Neumann architecture',\n",
       " 'Predication (computer architecture)',\n",
       " 'Computer',\n",
       " 'Microarchitecture',\n",
       " 'Computer science',\n",
       " 'Computer architecture simulator']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RELATED_TITLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90948c3",
   "metadata": {},
   "source": [
    "## Dynamic subheading generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "92a2adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NounExtractor(text):\n",
    "    nouns = []\n",
    "    result = []\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        words = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        cleaned_sentence = []\n",
    "        for (word, tag) in tagged:\n",
    "            if tag != 'NN' and tag != 'NNP' and tag != 'NNS' and tag != 'NNPS': # If the word is a proper noun  \n",
    "                if word.isalnum():\n",
    "                    cleaned_sentence.append(word)\n",
    "            else:\n",
    "                if word.isalnum() and len(word) > 2:\n",
    "                    nouns.append(word)\n",
    "        result.append(' '.join(cleaned_sentence))\n",
    "    return nouns, ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6f49307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clustering_subtopics/clusters.pickle', 'rb') as handle:\n",
    "    clusters = pickle.load(handle)\n",
    "clustered_subtopics = list(clusters.values())\n",
    "clustered_subtopics_flat = [j for sub in clustered_subtopics for j in sub]\n",
    "clustered_subtopics_flat_clean = []\n",
    "for word in clustered_subtopics_flat:\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'_', ' ', word)\n",
    "    clustered_subtopics_flat_clean.append(word)\n",
    "clustered_subtopics_flat_clean = set(clustered_subtopics_flat_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "32708005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filtered or raw (dependent on raw argument) sub sections for an article's content\n",
    "def get_subsections(data, raw=False):\n",
    "    subsections = re.findall('\\n== ([a-zA-z ]+) ==', data)\n",
    "    if raw:\n",
    "        return [subsection.lower() for subsection in subsections]\n",
    "    \n",
    "    subsections = [clean_title(subsection) for subsection in subsections]\n",
    "    blacklisted_articles = [\"reference\", \"see also\", \"external link\", \"note\", \"further reading\"]\n",
    "    subsections = [subsection.lower() for subsection in subsections if subsection not in blacklisted_articles]\n",
    "    return subsections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3ed49449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts subsections and content from related pages (cleans formatting)\n",
    "def get_important_subsections_and_content(related_titles):\n",
    "    topics = []\n",
    "    related_paper_section_content = defaultdict(list)\n",
    "    for related_title in tqdm(related_titles):\n",
    "        # Get a WikipediaPage for every string title\n",
    "        try:\n",
    "            related_page = wiki.WikipediaPage(title=related_title)\n",
    "        except wiki.DisambiguationError as e:\n",
    "            continue\n",
    "\n",
    "        content = (related_page.content).lower()\n",
    "        subsections = get_subsections(content, raw=True)\n",
    "        topics.extend(get_subsections(content))\n",
    "        delimiters = ''\n",
    "        for subsection in subsections:\n",
    "            delimiters += '== ' + str(subsection) + ' ==|'\n",
    "        delimiters = delimiters[:-1]\n",
    "        words = re.split(delimiters, content)\n",
    "        words = [word.replace('\\n', '') for word in words]\n",
    "        words = str(words[0])\n",
    "        nouns, words = NounExtractor(words)\n",
    "        important_nouns.extend(nouns)\n",
    "        related_paper_section_content['intro'].append(str(words[0]))\n",
    "        for i, subsection in enumerate(subsections):\n",
    "            related_paper_section_content[subsection].append(str(words[i+1]))\n",
    "\n",
    "    \n",
    "    topics_set = set(topics).intersection(clustered_subtopics_flat_clean)\n",
    "    common_subsections = Counter(list(topics_set))\n",
    "    important_subsections = heapq.nlargest(SUBSECTIONS, common_subsections, key=common_subsections.__getitem__)\n",
    "    return important_subsections, related_paper_section_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "70587530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:08<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subheadings are:  ['hardware', 'overview', 'philosophy', 'background']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run related_paper_section_content separately <- makes it faster\n",
    "important_nouns = []\n",
    "important_subsections, related_paper_section_content = get_important_subsections_and_content(RELATED_TITLES)\n",
    "# important_subsections.insert(0, 'intro')\n",
    "print(\"The subheadings are: \", important_subsections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "25f80f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important_subsections = ['history', 'source', 'type', 'overview', 'intoduction', 'examples', 'applications', 'syntax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a805c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the clusters to prevent this step from taking subtopics which are in the same \"cluster\" so they will be\n",
    "# quite different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b8329e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_counter = Counter(important_nouns)\n",
    "words_related_to_topic = heapq.nlargest(10, nouns_counter, key=nouns_counter.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fb0700a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computer',\n",
       " 'word',\n",
       " 'architecture',\n",
       " 'computers',\n",
       " 'instruction',\n",
       " 'design',\n",
       " 'data',\n",
       " 'devices',\n",
       " 'systems',\n",
       " 'unit']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_related_to_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19051e3c",
   "metadata": {},
   "source": [
    "## Subheading encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5de9679c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initializaiton:  5.173375844955444\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "print(\"Model initializaiton: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e15251a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subheading embeddings generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "topic_embeddings = defaultdict(list)\n",
    "# Creates word embeddings for subsection headings\n",
    "features = []\n",
    "for subsections in tqdm(important_subsections):\n",
    "    paras = related_paper_section_content[subsections]\n",
    "    topic_emb = np.average(model.encode(paras), 0)\n",
    "    features.append(list(topic_emb))\n",
    "features_tensor = torch.tensor(features)\n",
    "print(\"Subheading embeddings generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bb37ea3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55722e4d",
   "metadata": {},
   "source": [
    "## Format of output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9b62fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#     website_1: { intro: [para1, para7] -> summarize into 1-2 sentences\n",
    "#                  history: [...],   \n",
    "#                  ...\n",
    "#     }\n",
    "#     website_2: ...\n",
    "#     .\n",
    "#     .\n",
    "#     .\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# {\n",
    "#     intro: { website_1 : []\n",
    "        \n",
    "#     }\n",
    "    \n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2610e7",
   "metadata": {},
   "source": [
    "## Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "09bd1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(url):\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        \n",
    "def google_search(query):\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    response = get_source(\"https://www.google.com/search?q=\" + query)\n",
    "    links = list(set(response.html.absolute_links))\n",
    "    # Get rid of these from the domains that are used\n",
    "    google_domains = ('https://www.google.'\n",
    "                      'https://google.',\n",
    "                      'https://www.google.com/search?',\n",
    "                      'https://webcache.googleusercontent.', \n",
    "                      'http://webcache.googleusercontent.', \n",
    "                      'https://policies.google.',\n",
    "                      'https://support.google.',\n",
    "                      'https://maps.google.',\n",
    "                      'https://www.coursera.org',\n",
    "                      'https://www.youtube.com',\n",
    "                     'https://online.umich.edu/',\n",
    "                      'https://docs.oracle.com/',\n",
    "                      'https://www.cise.ufl.edu/~mssz/CompOrg/CDA-lang.html',\n",
    "                      'https://study.com/academy',\n",
    "                      'https://www.redhat.com',\n",
    "                      'https://www.oreilly.com',\n",
    "                      'https://scholar.google.com',\n",
    "                      'https://machinelearningknowledge',\n",
    "                      'https://interestingengineering.com',\n",
    "                      'https://www.nature.com/',\n",
    "                      'https://machinelearningmastery.com',\n",
    "                      'https://www.thelancet.com/',\n",
    "                      'https://m.youtube.com',\n",
    "                      'https://www.mathworks.com',\n",
    "                      'https://www.deeplearningbook',\n",
    "                      'https://u.today',\n",
    "                      'https://docplayer.net',\n",
    "                      'https://translate.google.com',\n",
    "                      'http://51.91.248.81',\n",
    "                      'https://twitter'\n",
    "                     )\n",
    "    \n",
    "    for url in links[:]:\n",
    "        url_check = url.split('#')[0]\n",
    "        if url_check in urls_visited or url.startswith(google_domains):\n",
    "            links.remove(url)\n",
    "        if url_check not in urls_visited:\n",
    "            urls_visited.add(url_check)\n",
    "        if url[-3:] == 'pdf':\n",
    "            links.remove(url)\n",
    "    return links\n",
    "\n",
    "# deprecated\n",
    "def collect_data_from_url(results):\n",
    "    data = []\n",
    "    for url in results:\n",
    "        print(url)\n",
    "        # Specially collect data from wikipedia\n",
    "        if url.startswith('http://en.wikipedia.org/wiki/') or url.startswith('https://en.wikipedia.org/wiki/'):\n",
    "            search_term = url.replace('http://en.wikipedia.org/wiki/', '').replace('https://en.wikipedia.org/wiki/', '').replace('_', ' ').replace('%E2%80%93', '-').replace('%27', \"'\")\n",
    "            sentences = wiki.WikipediaPage(title=search_term).content\n",
    "            text_info = ''\n",
    "            for sent in sentences.split('.'):\n",
    "                if sent == '' or len(sent) > 500 or len(sent) < 10:\n",
    "                    continue\n",
    "                sent_emb = torch.from_numpy(model.encode(sent))\n",
    "                if float(sent_emb @ topic_emb) < 0.3:\n",
    "                    continue\n",
    "                text_info += (sent + '. ')\n",
    "\n",
    "            item = {\n",
    "                'title': search_term,\n",
    "                'link': url,\n",
    "                'text': text_info,\n",
    "                'emb': model.encode(text_info)\n",
    "            }\n",
    "            data.append(item) \n",
    "        else:\n",
    "            try: \n",
    "                page = requests.get(url, timeout=(5, 10))\n",
    "            except requests.exceptions.Timeout as err: \n",
    "                #print(\"here\")\n",
    "                continue\n",
    "            #print(page)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\",from_encoding=\"iso-8859-1\")\n",
    "            p = soup.find_all('p')\n",
    "            paragraphs = []\n",
    "            for x in p:\n",
    "                paragraphs.append(str(x))\n",
    "            if len(paragraphs) == 0:\n",
    "                continue\n",
    "            text_info = ''\n",
    "            for para in paragraphs:\n",
    "                if para == '':\n",
    "                    continue\n",
    "                sentences = html_text.extract_text(para, guess_layout=False)\n",
    "                for sent in sentences.split('.'):\n",
    "                    if sent == '' or len(sent) > 500 or len(sent) < 10:\n",
    "                        continue\n",
    "                    sent_emb = torch.from_numpy(model.encode(sent))\n",
    "                    if float(sent_emb @ topic_emb) < 0.3:\n",
    "                        continue\n",
    "                    text_info += (sent + '. ')\n",
    "            if len(text_info) < 100 or len(text_info) > 10000: \n",
    "                continue\n",
    "            item = {\n",
    "                'title': \"<UNK>\",\n",
    "                'link': url,\n",
    "                'text': text_info,\n",
    "                'emb': model.encode(text_info)\n",
    "            }\n",
    "            data.append(item)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f6e5583e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have selected  31  webpages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "urls_visited = set()\n",
    "results = []\n",
    "for text in tqdm(important_subsections):\n",
    "    if text == 'intro':\n",
    "        results.extend(google_search(\"what is \" + TITLE))\n",
    "    else:\n",
    "        results.extend(google_search(TITLE + \" \" + text.lower()))\n",
    "\n",
    "print(\"We have selected \", len(results), \" webpages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5b249289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [00:17<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have scraped  409  paragraphs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "# raw_dataset = defaultdict(list)\n",
    "# count_of_paras = 0\n",
    "# # Get website urls and the paragraph tags in them\n",
    "# for result in tqdm(results):\n",
    "#     temp_dataset = ''\n",
    "#     paragraphs = []\n",
    "#     temp_cleaned_para = []\n",
    "#     try:\n",
    "#         page = requests.get(result, timeout=(5, 10), headers=headers)\n",
    "#     except:\n",
    "#         continue\n",
    "#     soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "#     p = soup.find_all('p')\n",
    "    \n",
    "#     for x in p:\n",
    "#         paragraphs.append(str(x))\n",
    "#     for i, para in enumerate(paragraphs):\n",
    "#         if para != '':\n",
    "#             temp_cleaned_para.append(html_text.extract_text(para, guess_layout=False))\n",
    "\n",
    "#     for i, para in enumerate(temp_cleaned_para):\n",
    "#         if len(nltk.word_tokenize(para)) > 30 and len(nltk.word_tokenize(para)) < 150:\n",
    "#             para = re.sub('[\\[].*?[\\]]', '', para)\n",
    "#             raw_dataset[result].append(para)\n",
    "#             count_of_paras += 1\n",
    "# print(\"We have scraped \", count_of_paras, \" paragraphs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f4ba030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(result):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    raw_dataset = defaultdict(list)\n",
    "    temp_dataset = ''\n",
    "    paragraphs = []\n",
    "    temp_cleaned_para = []\n",
    "    try:\n",
    "        page = requests.get(result, timeout=(5, 10), headers=headers)\n",
    "    except:\n",
    "        print(\"sad\")\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    p = soup.find_all('p')\n",
    "\n",
    "    for x in p:\n",
    "        paragraphs.append(str(x))\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        if para != '':\n",
    "            temp_cleaned_para.append(html_text.extract_text(para, guess_layout=False))\n",
    "\n",
    "    for i, para in enumerate(temp_cleaned_para):\n",
    "        if len(nltk.word_tokenize(para)) > 30 and len(nltk.word_tokenize(para)) < 150:\n",
    "            para = re.sub('[\\[].*?[\\]]', '', para)\n",
    "            raw_dataset[result].append(para)\n",
    "    return raw_dataset\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d24c20c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "3.9242477416992188\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "start = time.time()\n",
    "results = Parallel(n_jobs=10)(delayed(run)(result) for result in results)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "47484cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = defaultdict(list)\n",
    "for dic in results:\n",
    "    raw_dataset.update(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e540759b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ad36f356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'https://eng.libretexts.org/Bookshelves/Computer_Science/Programming_Languages/Book%3A_Python_for_Everybody_(Severance)/01%3A_Introduction/1.03%3A_Computer_Hardware_Architecture': ['Before we start learning the language we speak to give instructions to computers to develop software, we need to learn a small amount about how computers are built. If you were to take apart your computer or cell phone and look deep inside, you would find the following parts:',\n",
       "              'While most of the detail of how these components work is best left to computer builders, it helps to have some terminology so we can talk about these different parts as we write our programs.',\n",
       "              'As a programmer, your job is to use and orchestrate each of these resources to solve the problem that you need to solve and analyze the data you get from the solution. As a programmer you will mostly be \"talking\" to the CPU and telling it what to do next. Sometimes you will tell the CPU to use the main memory, secondary memory, network, or the input/output devices.',\n",
       "              'You need to be the person who answers the CPU\\'s \"What next?\" question. But it would be very uncomfortable to shrink you down to 5mm tall and insert you into the computer just so you could issue a command three billion times per second. So instead, you must write down your instructions in advance. We call these stored instructions a program and the act of writing these instructions down and getting the instructions to be correct programming.',\n",
       "              'The LibreTexts libraries are Powered by NICE CXone Expert and are supported by the Department of Education Open Textbook Pilot Project, the UC Davis Office of the Provost, the UC Davis Library, the California State University Affordable Learning Solutions Program, and Merlot. We also acknowledge previous National Science Foundation support under grant numbers 1246120, 1525057, and 1413739. Legal. Have questions or comments? For more information contact us at info@libretexts.org or check out our status page at https://status.libretexts.org.'],\n",
       "             'http://www2.latech.edu/~choi/Bens/Teaching/Csc364/index.htm': ['Architecture and organization of computer systems. Topics include the processor, control unit and microprogramming, computer arithmetic, memory hierarchy and memory management, input/output, and instruction sets.'],\n",
       "             'https://en.wikipedia.org/wiki/Computer_architecture': ['In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. The architecture of a system refers to its structure in terms of separately specified components of that system and their interrelationships. ',\n",
       "              'Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation.  In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation. ',\n",
       "              'The first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine. When building the computer Z1 in 1936, Konrad Zuse described in two patent applications for his future projects that machine instructions could be stored in the same storage used for data, i.e., the stored-program concept.   Two other early and important examples are:',\n",
       "              \"The term “architecture” in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959. Johnson had the opportunity to write a proprietary research communication about the Stretch, an IBM-developed supercomputer for Los Alamos National Laboratory (at the time known as Los Alamos Scientific Laboratory). To describe the level of detail for discussing the luxuriously embellished computer, he noted that his description of formats, instruction types, hardware parameters, and speed enhancements were at the level of “system architecture”, a term that seemed more useful than “machine organization”. \",\n",
       "              'Subsequently, Brooks, a Stretch designer, opened Chapter 2 of a book called Planning a Computer System: Project Stretch by stating, “Computer architecture, like other architecture, is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints.” ',\n",
       "              'Brooks went on to help develop the IBM System/360 (now called the IBM zSeries) line of computers, in which “architecture” became a noun defining “what the user needs to know”.  Later, computer users came to use the term in many less explicit ways. ',\n",
       "              'The earliest computer architectures were designed on paper and then directly built into the final hardware form.  Later, computer architecture prototypes were physically built in the form of a transistor–transistor logic (TTL) computer—such as the prototypes of the 6800 and the PA-RISC —tested, and tweaked, before committing to the final hardware form. As of the 1990s, new computer architectures are typically \"built\", tested, and tweaked—inside some other computer architecture in a computer architecture simulator; or inside a FPGA as a soft microprocessor; or both—before committing to the final hardware form. ',\n",
       "              'There are other technologies in computer architecture. The following technologies are used in bigger companies like Intel, and were estimated in 2002  to count for 1% of all of computer architecture:',\n",
       "              'Computer architecture is concerned with balancing the performance, efficiency, cost, and reliability of a computer system. The case of instruction set architecture can be used to illustrate the balance of these competing factors. More complex instruction sets enable programmers to write more space efficient programs, since a single instruction can encode some higher-level abstraction (such as the x86 Loop instruction).  However, longer and more complex instructions take longer for the processor to decode and can be more costly to implement effectively. The increased complexity from a large instruction set also creates more room for unreliability when instructions interact in unexpected ways.',\n",
       "              'The implementation involves integrated circuit design, packaging, power, and cooling. Optimization of the design requires familiarity with compilers, operating systems to logic design, and packaging. ',\n",
       "              \"An instruction set architecture (ISA) is the interface between the computer's software and hardware and also can be viewed as the programmer's view of the machine. Computers do not understand high-level programming languages such as Java, C++, or most programming languages used. A processor only understands instructions encoded in some numerical fashion, usually as binary numbers. Software tools, such as compilers, translate those high level languages into instructions that the processor can understand.\",\n",
       "              'Besides instructions, the ISA defines items in the computer that are available to a program—e.g., data types, registers, addressing modes, and memory. Instructions locate these available items with register indexes (or names) and memory addressing modes.',\n",
       "              'The ISA of a computer is usually described in a small instruction manual, which describes how the instructions are encoded. Also, it may define short (vaguely) mnemonic names for the instructions. The names can be recognized by a software development tool called an assembler. An assembler is a computer program that translates a human-readable form of the ISA into a computer-readable form. Disassemblers are also widely available, usually in debuggers and software programs to isolate and correct malfunctions in binary computer programs.',\n",
       "              'ISAs vary in quality and completeness. A good ISA compromises between programmer convenience (how easy the code is to understand), size of the code (how much code is required to do a specific action), cost of the computer to interpret the instructions (more complexity means more hardware needed to decode and execute the instructions), and speed of the computer (with more complex decoding hardware comes longer decode time). Memory organization defines how instructions interact with the memory, and how memory interacts with itself.',\n",
       "              'During design emulation, emulators can run programs written in a proposed instruction set. Modern emulators can measure size, cost, and speed to determine whether a particular ISA is meeting its goals.',\n",
       "              \"Computer organization helps optimize performance-based products. For example, software engineers need to know the processing power of processors. They may need to optimize software in order to gain the most performance for the lowest price. This can require quite a detailed analysis of the computer's organization. For example, in an SD card, the designers might need to arrange the card so that the most data can be processed in the fastest possible way.\",\n",
       "              'Computer organization also helps plan the selection of a processor for a particular project. Multimedia projects may need very rapid data access, while virtual machines may need fast interrupts. Sometimes certain tasks need additional components as well. For example, a computer capable of running a virtual machine needs virtual memory hardware so that the memory of different virtual computers can be kept separated. Computer organization and features also affect power consumption and processor cost.',\n",
       "              'Once an instruction set and micro-architecture have been designed, a practical machine must be developed. This design process is called the implementation. Implementation is usually not considered architectural design, but rather hardware design engineering. Implementation can be further broken down into several steps:',\n",
       "              'The exact form of a computer system depends on the constraints and goals. Computer architectures usually trade off standards, power versus performance, cost, memory capacity, latency (latency is the amount of time that it takes for information from one node to travel to the source) and throughput. Sometimes other considerations, such as features, size, weight, reliability, and expandability are also factors.',\n",
       "              'Modern computer performance is often described in instructions per cycle (IPC), which measures the efficiency of the architecture at any clock frequency; a faster IPC rate means the computer is faster. Older computers had IPC counts as low as 0.1 while modern processors easily reach near 1. Superscalar processors may reach three to five IPC by executing several instructions per clock cycle. ',\n",
       "              'Counting machine-language instructions would be misleading because they can do varying amounts of work in different ISAs. The \"instruction\" in the standard measurements is not a count of the ISA\\'s machine-language instructions, but a unit of measurement, usually based on the speed of the VAX computer architecture.',\n",
       "              \"Many people used to measure a computer's speed by the clock rate (usually in MHz or GHz). This refers to the cycles per second of the main clock of the CPU. However, this metric is somewhat misleading, as a machine with a higher clock rate may not necessarily have greater performance. As a result, manufacturers have moved away from clock speed as a measure of performance.\",\n",
       "              'There are two main types of speed: latency and throughput. Latency is the time between the start of a process and its completion. Throughput is the amount of work done per unit time. Interrupt latency is the guaranteed maximum response time of the system to an electronic event (like when the disk drive finishes moving some data).',\n",
       "              'Performance is affected by a very wide range of design choices — for example, pipelining a processor usually makes latency worse, but makes throughput better. Computers that control machinery usually need low interrupt latencies. These computers operate in a real-time environment and fail if an operation is not completed in a specified amount of time. For example, computer-controlled anti-lock brakes must begin braking within a predictable and limited time period after the brake pedal is sensed or else failure of the brake will occur.',\n",
       "              \"Benchmarking takes all these factors into account by measuring the time a computer takes to run through a series of test programs. Although benchmarking shows strengths, it shouldn't be how you choose a computer. Often the measured machines split on different measures. For example, one system might handle scientific applications quickly, while another might render video games more smoothly. Furthermore, designers may target and add special features to their products, through hardware or software, that permit a specific benchmark to execute quickly but don't offer similar advantages to general tasks.\",\n",
       "              'Power efficiency is another important measurement in modern computers. A higher power efficiency can often be traded for lower speed or higher cost. The typical measurement when referring to power consumption in computer architecture is MIPS/W (millions of instructions per second per watt).',\n",
       "              'Modern circuits have less power required per transistor as the number of transistors per chip grows.  This is because each transistor that is put in a new chip requires its own power supply and requires new pathways to be built to power it. However the number of transistors per chip is starting to increase at a slower rate. Therefore, power efficiency is starting to become as important, if not more important than fitting more and more transistors into a single chip. Recent processor designs have shown this emphasis as they put more focus on power efficiency rather than cramming as many transistors into a single chip as possible.  In the world of embedded computers, power efficiency has long been an important goal next to throughput and latency.'],\n",
       "             'https://en.wikipedia.org/wiki/Computer_memory': ['In computing, memory is a device or system that is used to store information for immediate use in a computer or related computer hardware and digital electronic devices.  The term memory is often synonymous with the term primary storage or main memory. An archaic synonym for memory is store. ',\n",
       "              'Computer memory operates at a high speed compared to storage that is slower but less expensive and higher in capacity. Besides storing opened programs, computer memory serves as disk cache and write buffer to improve both reading and writing performance. Operating systems borrow RAM capacity for caching so long as not needed by running software.  If needed, contents of the computer memory can be transferred to storage; a common way of doing this is through a memory management technique called virtual memory.',\n",
       "              'Modern memory is implemented as semiconductor memory,   where data is stored within memory cells built from MOS transistors and other components on an integrated circuit.  There are two main kinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory are flash memory and ROM, PROM, EPROM and EEPROM memory. Examples of volatile memory are dynamic random-access memory (DRAM) used for primary storage, and static random-access memory (SRAM) used for CPU cache.',\n",
       "              'Most semiconductor memory is organized into memory cells each storing one bit (0 or 1). Flash memory organization includes both one bit per memory cell and multi-level cell capable of storing multiple bits per cell. The memory cells are grouped into words of fixed word length, for example, 1, 2, 4, 8, 16, 32, 64 or 128 bits. Each word can be accessed by a binary address of N bits, making it possible to store 2 N words in the memory.',\n",
       "              'In the early 1940s, memory technology often permitted a capacity of a few bytes. The first electronic programmable digital computer, the ENIAC, using thousands of vacuum tubes, could perform simple calculations involving 20 numbers of ten decimal digits stored in the vacuum tubes.',\n",
       "              'The next significant advance in computer memory came with acoustic delay-line memory, developed by J. Presper Eckert in the early 1940s. Through the construction of a glass tube filled with mercury and plugged at each end with a quartz crystal, delay lines could store bits of information in the form of sound waves propagating through the mercury, with the quartz crystals acting as transducers to read and write bits. Delay-line memory was limited to a capacity of up to a few thousand bits.',\n",
       "              'Two alternatives to the delay line, the Williams tube and Selectron tube, originated in 1946, both using electron beams in glass tubes as means of storage. Using cathode ray tubes, Fred Williams invented the Williams tube, which was the first random-access computer memory. The Williams tube was able to store more information than the Selectron tube (the Selectron was limited to 256 bits, while the Williams tube could store thousands) and less expensive. The Williams tube was nevertheless frustratingly sensitive to environmental disturbances.',\n",
       "              'Efforts began in the late 1940s to find non-volatile memory. Magnetic-core memory allowed for recall of memory after power loss. It was developed by Frederick W. Viehe and An Wang in the late 1940s, and improved by Jay Forrester and Jan A. Rajchman in the early 1950s, before being commercialised with the Whirlwind computer in 1953.  Magnetic-core memory was the dominant form of memory until the development of MOS semiconductor memory in the 1960s. ',\n",
       "              'The first semiconductor memory was implemented as a flip-flop circuit in the early 1960s using bipolar transistors.  Semiconductor memory made from discrete devices was first shipped by Texas Instruments to the United States Air Force in 1961. The same year, the concept of solid-state memory on an integrated circuit (IC) chip was proposed by applications engineer Bob Norman at Fairchild Semiconductor.  The first bipolar semiconductor memory IC chip was the SP95 introduced by IBM in 1965.  While semiconductor memory offered improved performance over magnetic-core memory, it remain larger and more expensive and did not displace magnetic-core memory until the late 1960s.  ',\n",
       "              'The two main types of volatile random-access memory (RAM) are static random-access memory (SRAM) and dynamic random-access memory (DRAM). Bipolar SRAM was invented by Robert Norman at Fairchild Semiconductor in 1963,  followed by the development of MOS SRAM by John Schmidt at Fairchild in 1964.  SRAM became an alternative to magnetic-core memory, but requires six transistors for each bit of data.  Commercial use of SRAM began in 1965, when IBM introduced their SP95 SRAM chip for the System/360 Model 95. ',\n",
       "              'Volatile memory is computer memory that requires power to maintain the stored information. Most modern semiconductor volatile memory is either static RAM (SRAM) or dynamic RAM (DRAM).  DRAM dominates for desktop system memory. SRAM is used for CPU cache. SRAM is also found in small embedded systems requiring little memory.',\n",
       "              'SRAM retains its contents as long as the power is connected and may use a simpler interface, but requires six transistors per bit. Dynamic RAM is more complicated for interfacing and control, needing regular refresh cycles to prevent losing its contents, but uses only one transistor and one capacitor per bit, allowing it to reach much higher densities and much cheaper per-bit costs.   ',\n",
       "              'Non-volatile memory can retain the stored information even when not powered. Examples of non-volatile memory include read-only memory, flash memory, most types of magnetic computer storage devices (e.g. hard disk drives, floppy disks and magnetic tape), optical discs, and early computer storage methods such as paper tape and punched cards. ',\n",
       "              'Non-volatile memory technologies under development include ferroelectric RAM, programmable metallization cell, Spin-transfer torque magnetic RAM, SONOS, resistive random-access memory, racetrack memory, Nano-RAM, 3D XPoint, and millipede memory.',\n",
       "              'A third category of memory is semi-volatile. The term is used to describe a memory that has some limited non-volatile duration after power is removed, but then data is ultimately lost. A typical goal when using a semi-volatile memory is to provide the high performance and durability associated with volatile memories while providing some benefits of non-volatile memory.',\n",
       "              'For example, some non-volatile memory types can wear out, where a worn cell has increased volatility but otherwise continues to work. Data locations which are written frequently can thus be directed to use worn circuits. As long as the location is updated within some known retention time, the data stays valid. If the retention time \"expires\" without an update, then the value is copied to a less-worn circuit with longer retention. Writing first to the worn area allows a high write rate while avoiding wear on the not-worn circuits. ',\n",
       "              'As a second example, an STT-RAM can be made non-volatile by building large cells, but the cost per bit and write power go up, while the write speed goes down. Using small cells improves cost, power, and speed, but leads to semi-volatile behavior. In some applications the increased volatility can be managed to provide many benefits of a non-volatile memory, for example by removing power but forcing a wake-up before data is lost; or by caching read-only data and discarding the cached data if the power-off time exceeds the non-volatile threshold. ',\n",
       "              'The term semi-volatile is also used to describe semi-volatile behavior constructed from other memory types. For example, a volatile and a non-volatile memory may be combined, where an external signal copies data from the volatile memory to the non-volatile memory, but if power is removed without copying, the data is lost. Or, a battery-backed volatile memory, and if external power is lost there is some known period where the battery can continue to power the volatile memory, but if power is off for an extended time, the battery runs down and data is lost. ',\n",
       "              'Proper management of memory is vital for a computer system to operate properly. Modern operating systems have complex systems to properly manage memory. Failure to do so can lead to bugs, slow performance, and at worst case, takeover by viruses and malicious software.',\n",
       "              'In early computer systems, programs typically specified the location to write memory and what data to put there. This location was a physical location on the actual memory hardware. The slow processing of such computers did not allow for the complex memory management systems used today. Also, as most such systems were single-task, sophisticated systems were not required as much.',\n",
       "              'This approach has its pitfalls. If the location specified is incorrect, this will cause the computer to write the data to some other part of the program. The results of an error like this are unpredictable. In some cases, the incorrect data might overwrite memory used by the operating system. Computer crackers can take advantage of this to create viruses and malware.',\n",
       "              \"Virtual memory is a system where all physical memory is controlled by the operating system. When a program needs memory, it requests it from the operating system. The operating system then decides in what physical location to place the program's code and data.\",\n",
       "              'Protected memory is a system where each program is given an area of memory to use and is not permitted to go outside that range. Use of protected memory greatly enhances both the reliability and security of a computer system.',\n",
       "              \"Without protected memory, it is possible that a bug in one program will alter the memory used by another program. This will cause that other program to run off of corrupted memory with unpredictable results. If the operating system's memory is corrupted, the entire computer system may crash and need to be rebooted. At times programs intentionally alter the memory used by other programs. This is done by viruses and malware to take over computers. It may also be used benignly by desirable programs which are intended to modify other programs; in the modern age, this is generally considered bad programming practice for application programs, but it may be used by system development tools such as debuggers, for example to insert breakpoints or hooks.\",\n",
       "              'Protected memory assigns programs their own areas of memory. If the operating system detects that a program has tried to alter memory that does not belong to it, the program is terminated (or otherwise restricted or redirected). This way, only the offending program crashes, and other programs are not affected by the misbehavior (whether accidental or intentional).'],\n",
       "             'https://en.wikipedia.org/wiki/Microarchitecture': ['In computer engineering, microarchitecture, also called computer organization and sometimes abbreviated as µarch or uarch, is the way a given instruction set architecture (ISA) is implemented in a particular processor.  A given ISA may be implemented with different microarchitectures;   implementations may vary due to different goals of a given design or due to shifts in technology. ',\n",
       "              'The ISA is roughly the same as the programming model of a processor as seen by an assembly language programmer or compiler writer. The ISA includes the instructions, execution model, processor registers, address and data formats among other things. The microarchitecture includes the constituent parts of the processor and how these interconnect and interoperate to implement the ISA.',\n",
       "              'The microarchitecture of a machine is usually represented as (more or less detailed) diagrams that describe the interconnections of the various microarchitectural elements of the machine, which may be anything from single gates and registers, to complete arithmetic logic units (ALUs) and even larger elements. These diagrams generally separate the datapath (where data is placed) and the control path (which can be said to steer the data). ',\n",
       "              'Each microarchitectural element is in turn represented by a schematic describing the interconnections of logic gates used to implement it. Each logic gate is in turn represented by a circuit diagram describing the connections of the transistors used to implement it in some particular logic family. Machines with different microarchitectures may have the same instruction set architecture, and thus be capable of executing the same programs. New microarchitectures and/or circuitry solutions, along with advances in semiconductor manufacturing, are what allows newer generations of processors to achieve higher performance while using the same ISA.',\n",
       "              'The pipelined datapath is the most commonly used datapath design in microarchitecture today. This technique is used in most modern microprocessors, microcontrollers, and DSPs. The pipelined architecture allows multiple instructions to overlap in execution, much like an assembly line. The pipeline includes several different stages which are fundamental in microarchitecture designs.  Some of these stages include instruction fetch, instruction decode, execute, and write back. Some architectures include other stages such as memory access. The design of pipelines is one of the central microarchitectural tasks.',\n",
       "              'Execution units are also essential to microarchitecture. Execution units include arithmetic logic units (ALU), floating point units (FPU), load/store units, branch prediction, and SIMD. These units perform the operations or calculations of the processor. The choice of the number of execution units, their latency and throughput is a central microarchitectural design task. The size, latency, throughput and connectivity of memories within the system are also microarchitectural decisions.',\n",
       "              'System-level design decisions such as whether or not to include peripherals, such as memory controllers, can be considered part of the microarchitectural design process. This includes decisions on the performance-level and connectivity of these peripherals.',\n",
       "              'Unlike architectural design, where achieving a specific performance level is the main goal, microarchitectural design pays closer attention to other constraints. Since microarchitecture design decisions directly affect what goes into a system, attention must be paid to issues such as chip area/cost, power consumption, logic complexity, ease of connectivity, manufacturability, ease of debugging, and testability.',\n",
       "              'Historically, the earliest computers were multicycle designs. The smallest, least-expensive computers often still use this technique. Multicycle architectures often use the least total number of logic elements and reasonable amounts of power. They can be designed to have deterministic timing and high reliability. In particular, they have no pipeline to stall when taking conditional branches or interrupts. However, other microarchitectures often perform more instructions per unit time, using the same logic family. When discussing \"improved performance,\" an improvement is often relative to a multicycle design.',\n",
       "              'In a multicycle computer, the computer does the four steps in sequence, over several cycles of the clock. Some designs can perform the sequence in two clock cycles by completing successive stages on alternate clock edges, possibly with longer operations occurring outside the main cycle. For example, stage one on the rising edge of the first cycle, stage two on the falling edge of the first cycle, etc.',\n",
       "              'Instruction sets have shifted over the years, from originally very simple to sometimes very complex (in various respects). In recent years, load–store architectures, VLIW and EPIC types have been in fashion. Architectures that are dealing with data parallelism include SIMD and Vectors. Some labels used to denote classes of CPU architectures are not particularly descriptive, especially so the CISC label; many early designs retroactively denoted \" CISC\" are in fact significantly simpler than modern RISC processors (in several respects).',\n",
       "              'However, the choice of instruction set architecture may greatly affect the complexity of implementing high-performance devices. The prominent strategy, used to develop the first RISC processors, was to simplify instructions to a minimum of individual semantic complexity combined with high encoding regularity and simplicity. Such uniform instructions were easily fetched, decoded and executed in a pipelined fashion and a simple strategy to reduce the number of logic levels in order to reach high operating frequencies; instruction cache-memories compensated for the higher operating frequency and inherently low code density while large register sets were used to factor out as much of the (slow) memory accesses as possible.',\n",
       "              'One of the first, and most powerful, techniques to improve performance is the use of instruction pipelining. Early processor designs would carry out all of the steps above for one instruction before moving onto the next. Large portions of the circuitry were left idle at any one step; for instance, the instruction decoding circuitry would be idle during execution and so on.',\n",
       "              'Pipelining improves performance by allowing a number of instructions to work their way through the processor at the same time. In the same basic example, the processor would start to decode (step 1) a new instruction while the last one was waiting for results. This would allow up to four instructions to be \"in flight\" at one time, making the processor look four times as fast. Although any one instruction takes just as long to complete (there are still four steps) the CPU as a whole \"retires\" instructions much faster.',\n",
       "              'RISC makes pipelines smaller and much easier to construct by cleanly separating each stage of the instruction process and making them take the same amount of time—one cycle. The processor as a whole operates in an assembly line fashion, with instructions coming in one side and results out the other. Due to the reduced complexity of the classic RISC pipeline, the pipelined core and an instruction cache could be placed on the same size die that would otherwise fit the core alone on a CISC design. This was the real reason that RISC was faster. Early designs like the SPARC and MIPS often ran over 10 times as fast as Intel and Motorola CISC solutions at the same clock speed and price.',\n",
       "              'Pipelines are by no means limited to RISC designs. By 1986 the top-of-the-line VAX implementation (VAX 8800) was a heavily pipelined design, slightly predating the first commercial MIPS and SPARC designs. Most modern CPUs (even embedded CPUs) are now pipelined, and microcoded CPUs with no pipelining are seen only in the most area-constrained embedded processors.  Large CISC machines, from the VAX 8800 to the modern Pentium 4 and Athlon, are implemented with both microcode and pipelines. Improvements in pipelining and caching are the two major microarchitectural advances that have enabled processor performance to keep pace with the circuit technology on which they are based.',\n",
       "              'It was not long before improvements in chip manufacturing allowed for even more circuitry to be placed on the die, and designers started looking for ways to use it. One of the most common was to add an ever-increasing amount of cache memory on-die. Cache is very fast and expensive memory. It can be accessed in a few cycles as opposed to many needed to \"talk\" to main memory. The CPU includes a cache controller which automates reading and writing from the cache. If the data is already in the cache it is accessed from there – at considerable time savings, whereas if it is not the processor is \"stalled\" while the cache controller reads it in.',\n",
       "              'RISC designs started adding cache in the mid-to-late 1980s, often only 4 KB in total. This number grew over time, and typical CPUs now have at least 2 MB, while more powerful CPUs come with 4 or 6 or 12MB or even 32MB or more, with the most being 768MB in the newly released EPYC Milan-X line, organized in multiple levels of a memory hierarchy. Generally speaking, more cache means more performance, due to reduced stalling.',\n",
       "              \"Caches and pipelines were a perfect match for each other. Previously, it didn't make much sense to build a pipeline that could run faster than the access latency of off-chip memory. Using on-chip cache memory instead, meant that a pipeline could run at the speed of the cache access latency, a much smaller length of time. This allowed the operating frequencies of processors to increase at a much faster rate than that of off-chip memory.\",\n",
       "              'Even with all of the added complexity and gates needed to support the concepts outlined above, improvements in semiconductor manufacturing soon allowed even more logic gates to be used.',\n",
       "              'In the outline above the processor processes parts of a single instruction at a time. Computer programs could be executed faster if multiple instructions were processed simultaneously. This is what superscalar processors achieve, by replicating functional units such as ALUs. The replication of functional units was only made possible when the die area of a single-issue processor no longer stretched the limits of what could be reliably manufactured. By the late 1980s, superscalar designs started to enter the market place.',\n",
       "              'In modern designs it is common to find two load units, one store (many instructions have no results to store), two or more integer math units, two or more floating point units, and often a SIMD unit of some sort. The instruction issue logic grows in complexity by reading in a huge list of instructions from memory and handing them off to the different execution units that are idle at that point. The results are then collected and re-ordered at the end.',\n",
       "              'The addition of caches reduces the frequency or duration of stalls due to waiting for data to be fetched from the memory hierarchy, but does not get rid of these stalls entirely. In early designs a cache miss would force the cache controller to stall the processor and wait. Of course there may be some other instruction in the program whose data is available in the cache at that point. Out-of-order execution allows that ready instruction to be processed while an older instruction waits on the cache, then re-orders the results to make it appear that everything happened in the programmed order. This technique is also used to avoid other operand dependency stalls, such as an instruction awaiting a result from a long latency floating-point operation or other multi-cycle operations.',\n",
       "              'Register renaming refers to a technique used to avoid unnecessary serialized execution of program instructions because of the reuse of the same registers by those instructions. Suppose we have two groups of instruction that will use the same register. One set of instructions is executed first to leave the register to the other set, but if the other set is assigned to a different similar register, both sets of instructions can be executed in parallel (or) in series.',\n",
       "              'Computer architects have become stymied by the growing mismatch in CPU operating frequencies and DRAM access times. None of the techniques that exploited instruction-level parallelism (ILP) within one program could make up for the long stalls that occurred when data had to be fetched from main memory. Additionally, the large transistor counts and high operating frequencies needed for the more advanced ILP techniques required power dissipation levels that could no longer be cheaply cooled. For these reasons, newer generations of computers have started to exploit higher levels of parallelism that exist outside of a single program or program thread.',\n",
       "              'This trend is sometimes known as throughput computing. This idea originated in the mainframe market where online transaction processing emphasized not just the execution speed of one transaction, but the capacity to deal with massive numbers of transactions. With transaction-based applications such as network routing and web-site serving greatly increasing in the last decade, the computer industry has re-emphasized capacity and throughput issues.',\n",
       "              'One technique of how this parallelism is achieved is through multiprocessing systems, computer systems with multiple CPUs. Once reserved for high-end mainframes and supercomputers, small-scale (2–8) multiprocessors servers have become commonplace for the small business market. For large corporations, large scale (16–256) multiprocessors are common. Even personal computers with multiple CPUs have appeared since the 1990s.',\n",
       "              \"With further transistor size reductions made available with semiconductor technology advances, multi-core CPUs have appeared where multiple CPUs are implemented on the same silicon chip. Initially used in chips targeting embedded markets, where simpler and smaller CPUs would allow multiple instantiations to fit on one piece of silicon. By 2005, semiconductor technology allowed dual high-end desktop CPUs CMP chips to be manufactured in volume. Some designs, such as Sun Microsystems ' UltraSPARC T1 have reverted to simpler (scalar, in-order) designs in order to fit more processors on one piece of silicon.\",\n",
       "              'Another technique that has become more popular recently is multithreading. In multithreading, when the processor has to fetch data from slow system memory, instead of stalling for the data to arrive, the processor switches to another program or program thread which is ready to execute. Though this does not speed up a particular program/thread, it increases the overall system throughput by reducing the time the CPU is idle.',\n",
       "              'Conceptually, multithreading is equivalent to a context switch at the operating system level. The difference is that a multithreaded CPU can do a thread switch in one CPU cycle instead of the hundreds or thousands of CPU cycles a context switch normally requires. This is achieved by replicating the state hardware (such as the register file and program counter) for each active thread.'],\n",
       "             'https://en.wikipedia.org/wiki/Hardware_architecture': [\"In engineering, hardware architecture refers to the identification of a system's physical components and their interrelationships. This description, often called a hardware design model, allows hardware designers to understand how their components fit into a system architecture and provides to software component designers important information needed for software development and integration. Clear definition of a hardware architecture allows the various traditional engineering disciplines (e.g., electrical and mechanical engineering) to work more effectively together to develop and manufacture new machines, devices and components. \",\n",
       "              'Hardware architecture is the representation of an engineered (or to be engineered) electronic or electromechanical hardware system, and the process and discipline for effectively implementing the design (s) for such a system. It is generally part of a larger integrated system encompassing information, software, and device prototyping. ',\n",
       "              'It is a representation because it is used to convey information about the related elements comprising a hardware system, the relationships among those elements, and the rules governing those relationships.',\n",
       "              'It is a process because a sequence of steps is prescribed to produce or change the architecture, and/or a design from that architecture, of a hardware system within a set of constraints.',\n",
       "              \"A hardware architecture is primarily concerned with the internal electrical (and, more rarely, the mechanical) interfaces among the system's components or subsystems, and the interface between the system and its external environment, especially the devices operated by or the electronic displays viewed by a user. (This latter, special interface, is known as the computer human interface, AKA human computer interface, or HCI; formerly called the man-machine interface.)  Integrated circuit (IC) designers are driving current technologies into innovative approaches for new products. Hence, multiple layers of active devices are being proposed as single chip, opening up opportunities for disruptive microelectronic, optoelectronic, and new microelectromechanical hardware implementation.  \",\n",
       "              'Prior to the advent of digital computers, the electronics and other engineering disciplines used the terms system and hardware as they are still commonly used today. However, with the arrival of digital computers on the scene and the development of software engineering as a separate discipline, it was often necessary to distinguish among engineered hardware artifacts, software artifacts, and the combined artifacts.',\n",
       "              'A programmable hardware artifact, or machine, that lacks its computer program is impotent; even as a software artifact, or program, is equally impotent unless it can be used to alter the sequential states of a suitable (hardware) machine. However, a hardware machine and its programming can be designed to perform an almost illimitable number of abstract and physical tasks. Within the computer and software engineering disciplines (and, often, other engineering disciplines, such as communications), then, the terms hardware, software, and system came to distinguish between the hardware that runs a computer program, the software, and the hardware device complete with its program.',\n",
       "              'The hardware engineer or architect deals (more or less) exclusively with the hardware device; the software engineer or architect deals (more or less) exclusively with the program; and the systems engineer or systems architect is responsible for seeing that the programming is capable of properly running within the hardware device, and that the system composed of the two entities is capable of properly interacting with its external environment, especially the user, and performing its intended function.',\n",
       "              'A hardware architecture, then, is an abstract representation of an electronic or an electromechanical device capable of running a fixed or changeable program.  '],\n",
       "             'https://www.technologyuk.net/computing/computer-hardware/architecture.shtml': [\"Computer architecture deals with the logical and physical design of a computer system. The Instruction Set Architecture (ISA) defines the set of machine-code instructions that the computer's central processing unit can execute. The microarchitecture describes the design features and circuitry of the central processing unit itself. The system architecture (with which we are chiefly concerned in this section) determines the main hardware components that make up the physical computer system (including, of course, the central processing unit) and the way in which they are interconnected. The main components required for a computer system are listed below.\",\n",
       "              'In addition to these core components, in order to extend the functionality of the system and to provide a computing environment with which a human operator can more easily interact, additional components are required. These could include:',\n",
       "              'A distinction is usually made between the internal components of the system (those normally located inside the main enclosure or case) and the external components (those that connect to the internal components via an external interface. Examples of such external components, usually referred to as peripherals, include the keyboard, video display unit (monitor) and mouse. Other peripherals can include printers, scanners, external speakers, external disk drives and webcams, to name but a few. The Internal components usually (though not always) include one or more disk drives for fixed or removable storage media (magnetic disk or tape, optical media etc.) although the core computing function does not absolutely require them. The relationship between the elements that make up the core of the system is illustrated below.',\n",
       "              'The core system components are mounted on a backplane, more commonly referred to as a mainboard (or motherboard). The mainboard is a relatively large printed circuit board that provides the electronic channels (buses) that carry data and control signals between the various components, as well as the necessary interfaces (in the form of slots or sockets) to allow the CPU, Memory cards and other components to be plugged into the system. In most cases, the ROM chip is built in to the mainboard, and the CPU and RAM must be compatible with the mainboard in terms of their physical format and electronic configuration. Internal I/O ports are provided on the mainboard for devices such as internal disk drives and optical drives.',\n",
       "              'External I/O ports are also provided on the mainboard to enable the system to be connected to external peripheral devices such as the keyboard, mouse, video display unit, and audio speakers. Both the video adaptor and audio card may be provided \"on-board\" (i.e. built in to the mainboard), or as separate plug-in circuit boards that are mounted in an appropriate slot on the mainboard. The mainboard also provides much of the control circuitry required by the various system components, allowing the CPU to concentrate on its main role, which is to execute programs. We will be looking at the individual system components in detail in later sections.'],\n",
       "             'https://en.wikipedia.org/wiki/Word_(computer_architecture)': ['In computing, a word is the natural unit of data used by a particular processor design. A word is a fixed-sized datum handled as a unit by the instruction set or the hardware of the processor. The number of bits or digits  in a word (the word size, word width, or word length) is an important characteristic of any specific processor design or computer architecture.',\n",
       "              'The size of a word is reflected in many aspects of a computer\\'s structure and operation; the majority of the registers in a processor are usually word sized and the largest datum that can be transferred to and from the working memory in a single operation is a word in many (not all) architectures. The largest possible address size, used to designate a location in memory, is typically a hardware word (here, \"hardware word\" means the full-sized natural word of the processor, as opposed to any other definition used).',\n",
       "              'Documentation for computers with fixed word size commonly stated memory sizes in words rather than bytes or characters. The documentation sometimes used metric prefixes correctly, sometimes with rounding, e.g., 65 kilowords (KW) meaning for 65536 words, and sometimes used them incorrectly, with kilowords (KW) meaning 1024 words (2 10) and megawords (MW) meaning 1,048,576 words (2 20). With standardization on 8-bit bytes and byte addressability, stating memory sizes in bytes, kilobytes, and megabytes with powers of 1024 rather than 1000 has become the norm, although there is some use of the IEC binary prefixes.',\n",
       "              'Several of the earliest computers (and a few modern as well) used binary-coded decimal rather than plain binary, typically having a word size of 10 or 12 decimal digits, and some early decimal computers had no fixed word length at all. Early binary systems tended to use word lengths that were some multiple of 6-bits, with the 36-bit word being especially common on mainframe computers. The introduction of ASCII led to the move to systems with word lengths that were a multiple of 8-bits, with 16-bit machines being popular in the 1970s before the move to modern processors with 32 or 64 bits.  Special-purpose designs like digital signal processors, may have any word length from 4 to 80 bits. ',\n",
       "              'The size of a word can sometimes differ from the expected due to backward compatibility with earlier computers. If multiple compatible variations or a family of processors share a common architecture and instruction set but differ in their word sizes, their documentation and software may become notationally complex to accommodate the difference (see Size families below).',\n",
       "              'When a computer architecture is designed, the choice of a word size is of substantial importance. There are design considerations which encourage particular bit-group sizes for particular uses (e.g. for addresses), and these considerations point to different sizes for different uses. However, considerations of economy in design strongly push for one size, or a very few sizes related by multiples or fractions (submultiples) to a primary size. That preferred size becomes the word size of the architecture.',\n",
       "              'Character size was in the past (pre-variable-sized character encoding) one of the influences on unit of address resolution and the choice of word size. Before the mid-1960s, characters were most often stored in six bits; this allowed no more than 64 characters, so the alphabet was limited to upper case. Since it is efficient in time and space to have the word size be a multiple of the character size, word sizes in this period were usually multiples of 6 bits (in binary machines). A common choice then was the 36-bit word, which is also a good size for the numeric properties of a floating point format.',\n",
       "              'After the introduction of the IBM System/360 design, which used eight-bit characters and supported lower-case letters, the standard size of a character (or more accurately, a byte) became eight bits. Word sizes thereafter were naturally multiples of eight bits, with 16, 32, and 64 bits being commonly used.',\n",
       "              'Early machine designs included some that used what is often termed a variable word length. In this type of organization, an operand had no fixed length. Depending on the machine and the instruction, the length might be denoted by a count field, by a delimiting character, or by an additional bit called, e.g., flag, word mark. Such machines often used binary-coded decimal in 4-bit digits, or in 6-bit characters, for numbers. This class of machines included the IBM 702, IBM 705, IBM 7080, IBM 7010, UNIVAC 1050, IBM 1401, IBM 1620, and RCA 301.',\n",
       "              'Most of these machines work on one unit of memory at a time and since each instruction or datum is several units long, each instruction takes several cycles just to access memory. These machines are often quite slow because of this. For example, instruction fetches on an IBM 1620 Model I take 8 cycles just to read the 12 digits of the instruction (the Model II reduced this to 6 cycles, or 4 cycles if the instruction did not need both address fields). Instruction execution took a completely variable number of cycles, depending on the size of the operands.',\n",
       "              'The memory model of an architecture is strongly influenced by the word size. In particular, the resolution of a memory address, that is, the smallest unit that can be designated by an address, has often been chosen to be the word. In this approach, the word-addressable machine approach, address values which differ by one designate adjacent memory words. This is natural in machines which deal almost always in word (or multiple-word) units, and has the advantage of allowing instructions to use minimally sized fields to contain addresses, which can permit a smaller instruction size or a larger variety of instructions.',\n",
       "              'When byte processing is to be a significant part of the workload, it is usually more advantageous to use the byte, rather than the word, as the unit of address resolution. Address values which differ by one designate adjacent bytes in memory. This allows an arbitrary character within a character string to be addressed straightforwardly. A word can still be addressed, but the address to be used requires a few more bits than the word-resolution alternative. The word size needs to be an integer multiple of the character size in this organization. This addressing approach was used in the IBM 360, and has been the most common approach in machines designed since then.',\n",
       "              'When the workload involves processing fields of different sizes, it can be advantageous to address to the bit. Machines with bit addressing may have some instructions that use a programmer-defined byte size and other instructions that operate on fixed data sizes. As an example, on the IBM 7030  (\"Stretch\"), a floating point instruction can only address words while an integer arithmetic instruction can specify a field length of 1-64 bits, a byte size of 1-8 bits and an accumulator offset of 0-127 bits.',\n",
       "              'In at byte-addressable machine with storage-to-storage (SS) instructions, there are typically move instructions to copy one or multiple bytes from one arbitrary location to another. In a byte-oriented (byte-addressable) machine without SS instructions, moving a single byte from one arbitrary location to another is typically:',\n",
       "              'Individual bytes can be accessed on a word-oriented machine in one of two ways. Bytes can be manipulated by a combination of shift and mask operations in registers. Moving a single byte from one arbitrary location to another may require the equivalent of the following:',\n",
       "              'Alternatively many word-oriented machines implement byte operations with instructions using special byte pointers in registers or memory. For example, the PDP-10 byte pointer contained the size of the byte in bits (allowing different-sized bytes to be accessed), the bit position of the byte within the word, and the word address of the data. Instructions could automatically adjust the pointer to the next byte on, for example, load and deposit (store) operations.',\n",
       "              'Different amounts of memory are used to store data values with different degrees of precision. The commonly used sizes are usually a power of two multiple of the unit of address resolution (byte or word). Converting the index of an item in an array into the memory address offset of the item then requires only a shift operation rather than a multiplication. In some cases this relationship can also avoid the use of division operations. As a result, most modern computer designs have word sizes (and other operand sizes) that are a power of two times the size of a byte.',\n",
       "              'As computer designs have grown more complex, the central importance of a single word size to an architecture has decreased. Although more capable hardware can use a wider variety of sizes of data, market forces exert pressure to maintain backward compatibility while extending processor capability. As a result, what might have been the central word size in a fresh design has to coexist as an alternative size to the original word size in a backward compatible design. The original word size remains available in future designs, forming the basis of a size family.',\n",
       "              'In the mid-1970s, DEC designed the VAX to be a 32-bit successor of the 16-bit PDP-11. They used word for a 16-bit quantity, while longword referred to a 32-bit quantity; this terminology is the same as the terminology used for the PDP-11. This was in contrast to earlier machines, where the natural unit of addressing memory would be called a word, while a quantity that is one half a word would be called a halfword. In fitting with this scheme, a VAX quadword is 64 bits. They continued this 16-bit word/32-bit longword/64-bit quadword terminology with the 64-bit Alpha.',\n",
       "              'A similar phenomenon has developed in Intel\\'s x86 assembly language – because of the support for various sizes (and backward compatibility) in the instruction set, some instruction mnemonics carry \"d\" or \"q\" identifiers denoting \"double-\", \"quad-\" or \"double-quad-\", which are in terms of the architecture\\'s original 16-bit word size.',\n",
       "              'An example with a different word size is the IBM System/360 family. In the System/360 architecture, System/370 architecture and System/390 architecture, there are 8-bit byte s, 16-bit halfword s, 32-bit word s and 64-bit doubleword s. The z/Architecture, which is the 64-bit member of that architecture family, continues to refer to 16-bit halfword s, 32-bit word s, and 64-bit doubleword s, and additionally features 128-bit quadword s.',\n",
       "              'Often carefully written source code – written with source-code compatibility and software portability in mind – can be recompiled to run on a variety of processors, even ones with different data word lengths or different address widths or both.'],\n",
       "             'https://www.cs.utah.edu/~germain/PPS/Topics/architecture.html': ['Computers are built of various components (called Hardware). These include: the CPU, memory, busses, clocks, peripherals (printers, keyboards, etc), etc. All of these components must communicate with each other and provide a necessary service to the user.',\n",
       "              'A computer is a complex device containing many sub-systems. Each sub-system is a piece of computer hardware (or part of a piece of computer hardware.) How these pieces interact is called the \"Computer Architecture\". The hard of a computer includes:'],\n",
       "             'https://www.educba.com/types-of-computer-architecture/': ['Computer architecture consists of rules and methods or procedures which describe the implementation, functionality of the computer systems. Architecture is built as per the user’s needs by taking care of the economic and financial constraints. Earlier architecture is designed on paper built with hardware form.',\n",
       "              'After it is built-in transistor-transistor logic the architecture is built, tested and formed in the hardware form. We can define computer architecture based on its performance, efficiency, reliability, and cost of the computer system. It deals with software and hardware technology standards. The computer system has the processor, memory, I/O devices and communication channels that connect to it.',\n",
       "              'The memory we have a single read/write memory available for read and write instructions and data. When we talk about memory, it is nothing but the single location which is used for reading and writing instructions for the data and instructions are also present in it. Data and instructions are stored in a single read/write memory within the computer system.',\n",
       "              'Each memory has multiple locations and each location has a unique address. We can address the contents of memory by its location irrespective of what type of data and instructions are present in the memory, because of which we can read or write any data and instructions. Execution always occurs in a sequential manner unless the change is required. For example, suppose we are executing an instruction from line 1 to line 10 but now we required to execute line 50 instead of line 11 then we jump to instruction 50 and execute it.',\n",
       "              'There is a bus (address bus/data bus/control bus) used for the instruction and data code execution. Input device takes data or instruction and the Central processing unit (CPU) performs one operation at a time, either fetching data or instruction in/out of the memory. Once the operation is done it is sent to the output device. Control and logic units for processing operations are within the central processing unit.',\n",
       "              'Harvard architecture is used when data and code is present in different memory blocks. A separate memory block is needed for data and instruction. Data can be accessed by one memory location and instruction can be accessed by a different location. It has data storage entirely contained within the central processing unit (CPU). A single set of clock cycles is required. The pipeline is possible. It is complex to design. CPU can read and write instructions and process data access. Harvard architecture has different access codes and data address spaces that is, the instruction address zero is not the same as data address zero. Instruction address zero identifies 24-byte value and data address zero identifies 8-byte value which is not the part of the 24-byte value.',\n",
       "              'Modified harvard architecture is like a harvard architecture machine and it has a common address space for the separate data and instruction cache. It has digital signal processors that will execute small or highly audio or video algorithms and it is reproducible. Microcontrollers have a small number of programs and data memory and it speeds up the processing by executing parallel instructions and data access.',\n",
       "              'We can observe in the below image, there are separate data and instruction memory that is a bus available to perform operations. It is contained entirely within the Central processing unit. It can perform Input/output operation simultaneously and it has a separate arithmetic and logic unit.',\n",
       "              'To make up the architecture, instruction set architecture is needed because it has a set of instructions that the processor understands. It has two instruction set one is RISC (reduced instruction set computer) and the second is CISC (complex instruction set computer).',\n",
       "              'Reduced instruction set computer architecture was realized in the 90’s by IBM. Instruction has multiple address modes, but programs do not use all of them that is the reason multiple address modes were reduced. This helps the compiler to easily write the instructions, performed is increased.',\n",
       "              'Complex instruction set architecture is the root of compilers because earlier compilers were not there to write programs, to ease programming instructions are added. The best performance is obtained by using simple instruction from ISA.',\n",
       "              'Microarchitecture is known as computer organizations and it is the way when instruction set architecture is a built-in processor. Instruction set architecture is implemented with various microarchitecture and it varies because of changing technology.',\n",
       "              'Microarchitecture performs in a certain way. It reads the instruction and decodes it, will find parallel data to process the instruction and then will process the instruction and output will be generated.',\n",
       "              'It is used in microprocessors, microcontrollers. Some architectures overlap multiple instructions while executing but this does not happen in microarchitecture. Execution units like arithmetic logic units, floating-point units, load units, etc are needed and it performs the operation of the processor. There are microarchitecture decisions within the system such as size, latency, and connectivity of the memories.',\n",
       "              'The name defines itself, the design will satisfy user requirements such as architecture, module, interfaces and data for a system and it is connected to product development. It is the process of taking marketing information and creating product design to be manufacture. Modular systems are made by standardizing hardware and software.',\n",
       "              'We have learned about computer architecture and its types. How functionality, implementation works in processing. Instruction set architecture is needed to do the needful instruction execution and data processing should be done in a different and single memory location in different types of computer architectures. Read/write operations are performed.',\n",
       "              'This is a guide to Types of Computer Architecture. Here we discuss the basic concept and different types of computer architecture in detail. You may also have a look at the following articles to learn more –',\n",
       "              'This website or its third-party tools use cookies, which are necessary to its functioning and required to achieve the purposes illustrated in the cookie policy. By closing this banner, scrolling this page, clicking a link or continuing to browse otherwise, you agree to our Privacy Policy'],\n",
       "             'http://www.edwardbosworth.com/My5155Textbook_HTM/MyText5155_Ch09_V06.htm': ['We now begin an overview of the architecture of a typical stored program computer. It should be noted that this architecture is common to almost all computers running today, from the smallest industrial controller to the largest supercomputer. What sets the larger computers, such as the IBM ASCII Blue (a supercomputer capable of 10 15 floating point operations per second), apart from the typical PC is that many larger computers are built from a large number of processor and memory modules that communicate and work cooperatively on a problem. The basic architecture is the same.',\n",
       "              'Stored program computers have four major components: the CPU (Central Processing Unit), the memory, I/O devices, and one or more bus structures to allow the other three components to communicate. The figure below illustrates a typical architecture.',\n",
       "              'The functions of the three top-level components of a computer seem to be obvious. The I/O devices allow for communication of data to other devices and the users. The memory stores both program data and executable code in the form of binary machine language. The CPU comprises components that execute the machine language of the computer. Within the CPU, it is the function of the control unit to interpret the machine language and cause the CPU to execute the instructions as written. The Arithmetic Logic Unit (ALU) is that component of the CPU that does the arithmetic operations and the logical comparisons that are necessary for program execution. The ALU uses a number of local storage units, called registers, to hold results of its operations. The set of registers is sometimes called the register file.',\n",
       "              'As we shall see, the fetch-execute cycle forms the basis for operation of a stored-program computer. The CPU fetches each instruction from the memory unit, then executes that instruction, and fetches the next instruction. An exception to the “fetch next instruction” rule comes when the equivalent of a Jump or Go To instruction is executed, in which case the instruction at the indicated address is fetched and executed.',\n",
       "              'Registers and memory are similar in that both store data. The difference between the two is somewhat an artifact of the history of computation, which has become solidified in all current architectures. The basic difference between devices used as registers and devices used for memory storage is that registers are usually faster and more expensive (see below for a discussion of registers and Level–1 Cache).',\n",
       "              'In each of the designs above, the goal was the same – to reduce the number of “storage units” that required the expensive and hard-to-maintain vacuum tubes. This small number of storage units became the register file associated with the central processing unit (CPU). It was not until the MIT Whirlwind in 1952 that magnetic core memory was introduced.',\n",
       "              'Both memory and registers can be viewed as collections of D flip-flops, as discussed in a previous chapter. The main difference is that registers (as static memory) may actually be built from these flip-flops, while computer memory is fabricated from a different technology called dynamic memory. We often describe main memory as if it were fabricated from flip-flops as this leads to a model that is logically correct.',\n",
       "              'A flip-flop stores one bit of data. An N–bit register is a collection of N flip-flops; thus a 32–bit register is built from 32 flip-flops. The CPU contains two types of registers, called special purpose registers and general purpose registers. The general purpose registers contain data used in computations and can be accessed directly by the computer program. The special purpose registers are used by the control unit to hold temporary results, access memory, and sequence the program execution. Normally, with one now-obsolete exception, these registers cannot be accessed by the program.',\n",
       "              'The program status register (PSR), also called the program status word (PSW), is one of the special purpose registers found on most computers. The PSR contains a number of bits to reflect the state of the CPU as well as the result of the most recent computation. Some of the common bits are C the carry-out from the last arithmetic computation V Set to 1 if the last arithmetic operation resulted in an overflow N Set to 1 if the last arithmetic operation resulted in a negative number Z Set to 1 if the last arithmetic operation resulted in a zero I Interrupts enabled (Interrupts are discussed later)',\n",
       "              'The central processing unit contains four major elements 1) The ALU (Arithmetic Logic Unit), and 2) The control unit, and 3) The register file (including user registers and special-purpose registers), and 4) A set of buses used for communications within the CPU.',\n",
       "              'The next figure shows a better top-level view of the CPU, showing three data buses and an ALU optimized for standard arithmetic. Most arithmetic (consider addition: C = A + B) is based on production of a result from two arguments. To facilitate such operations, the ALU is designed with two inputs and a single output. As each input and output must be connected to a bus internal to the CPU, this dictates at least three internal CPU buses.',\n",
       "              'The register file contains a number of general-purpose registers accessible to the assembly language operations (often numbered 0 through some positive integer) and a number of special-purpose registers not directly accessed by the program. With numbered registers (say R0 through R7) it is often convenient to have R0 be identically 0. Such a constant register greatly simplifies the construction of the control unit.',\n",
       "              'MAR the memory address register contains the address of the word in main memory that is being accessed. The word being addressed contains either data or a machine language instruction to be executed.',\n",
       "              'Reading Memory First place an address in the MAR. Assert a READ control signal to command memory to be read. Wait for memory to produce the result. Copy the contents of the MBR to a register in the CPU.',\n",
       "              'Writing Memory First place and address in the MAR Copy the contents of a register in the CPU to the MBR. Assert a WRITE control signal to command the memory.',\n",
       "              'Copy the contents of the PC into the MAR. Assert a READ control signal to the memory. While waiting on the memory, increment the PC to point to the next instruction Copy the MBR into the IR. Decode the bits found in the IR to determine what the instruction says to do.',\n",
       "              'The control unit issues control signals that cause the CPU (and other components of the computer) to fetch the instruction to the IR (Instruction Register) and then execute the actions dictated by the machine language instruction that has been stored there. One might imagine the following sequence of control signals corresponding to the instruction fetch.',\n",
       "              'T0: PC to Bus1, Transfer Bus1 to Bus3, Bus3 to MAR, READ. T1: PC to Bus1, +1 to Bus2, Add, Bus3 to PC. T2: MBR to Bus2, Transfer Bus2 to Bus3, Bus3 to IR.',\n",
       "              'This simple sequence introduces a number of concepts that will be used later. 1. The internal buses of the CPU are named Bus1, Bus2, and Bus3. 2. All registers can transfer data to either Bus1 or Bus2. 3. Only Bus3 can transfer data into a register. 4. Only the ALU can transfer data from either Bus1 to Bus3 or Bus2 to Bus3. It does this by a specific transfer operation. 5. Control signals are named for the action that they cause to take place.',\n",
       "              'We now examine very briefly the two most common methods for building a control unit. Recall that the only function of the control unit is to emit control signals, so that the design of a control unit is just an investigation of how to generate control signals. There are two major classes of control units: hardwired and microprogrammed (or microcoded). In order to see the difference, let’s write the above control signals for the common fetch sequence in a more compact notation.',\n",
       "              'T0: PC ® Bus1, TRA1, Bus3 ® MAR, READ. T1: PC ® Bus1, +1 ® Bus2, ADD, Bus3 ® PC. T2: MBR ® Bus2, TRA2, Bus3 ® IR.',\n",
       "              'Here we have used ten control signals. Remember that the ALU has two inputs, one from Bus1, one from Bus2, and outputs its results on Bus3. The control signals used are:',\n",
       "              'PC ® Bus1 Copy the contents of the PC (Program Counter) onto Bus1 +1 ® Bus2 Copy the contents of the constant register +1 onto Bus2. MBR ® Bus2 Copy the contents of the MBR (Memory Buffer Register) onto Bus2 TRA1 Causes the ALU to copy the contents of Bus1 onto Bus3 TRA2 Causes the ALU to copy the contents of Bus2 onto Bus3 ADD Causes the ALU to add the contents of Bus1 and Bus2, placing the sum onto Bus3. READ Causes the memory to be read and place the results in the MBR Bus3 ® MAR Copy the contents of Bus3 to the MAR (Memory Address Register) Bus3 ® PC Copy the contents of Bus3 to the PC (Program Counter) Bus3 ® IR Copy the contents of Bus3 to the IR (Instruction Register)',\n",
       "              'All control units have a number of important inputs, including the system clock, the IR, the PSR (program status register) and other status and control signals. A hardwired control unit uses combinational logic to produce the output. The following shows how the above signals would be generated by a hardwired control unit.',\n",
       "              'Here we assume that we have the discrete signal FETCH, which is asserted during the fetch phase of the instruction processing, and discrete time signals T0, T1, and T2, which would be generated by a counter within the control unit. Note here that we already have a naming problem: there will be a distinct phase of the Fetch/Execute cycle called “FETCH”. During that cycle, the discrete signal FETCH will be active. This discrete signal is best viewed as a Boolean value, having only two values: Logic 1 (+5 volts) and Logic 0 (0 volts).',\n",
       "              'We next consider how a microprogrammed unit would generate the above signals. In this discussion, we shall present a simplified picture of such a control with a number of design options assumed; these will be explained later in the text.',\n",
       "              'The central part of a microprogrammed control unit is the micro-memory, which is used to store the microprogram (or microcode). The microprogram essentially interprets the machine language instructions in that it causes the correct control signals to be emitted in the correct sequence. The microprogram, written in microcode, is stored in a read-only memory (ROM, PROM, or EPROM), which is often called the control store.',\n",
       "              'A microprogrammed control unit functions by reading a sequence of control words into a microinstruction buffer that is used to convert the binary bits in the microprogram into control signals for use by the CPU. To do this, there are several other components',\n",
       "              'the m MAR the micro-address of the next control word to read the m MBR this holds the last control word read from the micro-memory the sequencer this computes the next value of the address for the m MAR.',\n",
       "              'A typical computer contains a number of bus structures. We have already mentioned the system bus and a bus internal to the CPU. Some computer designs include high-speed point-to-point busses, used for such tasks as communication to the graphics card. In this section, we consider the structure of the system bus. The system bus is a multi-point bus that allows communication between a number of devices that are attached to the bus. There are two classes of devices that can be connected to the bus',\n",
       "              'Master Device a device that can initiate action on the bus. The CPU is always a bus master. Slave Device a device that responds to requests by a bus master. Memory is an excellent example of a slave device.',\n",
       "              'Devices connected to a bus are often accessed by address. System memory is a primary example of an addressable device; in a byte-addressable machine (more later on this), memory can be considered as an array of bytes, accessed in the same way as an array as seen in a typical programming language. I/O devices are often accessed by address; it is up to the operating system to know the address used to access each such device.',\n",
       "              'A typical bus can be considered as a number of wires (called lines) that act as a common path to connect the devices. The lines fall into a number of major classes',\n",
       "              'Data lines used to transfer the data between the two devices Address lines used to identify the device or memory location to which the data are written or from which the data are read Control lines used to indicate what operation is to be done Power & Ground used to provide a common power and common ground.',\n",
       "              'For a foray into the real world (or the world that once was real), we quote from two publications describing the UNIBUS™ on a PDP-11, a minicomputer marketed by the Digital Equipment Corporation (DEC – now a part of Hewlett-Packard). We quote from three manuals published by DEC.',\n",
       "              '“UNIBUS The single, asynchronous, high-speed bus structure shared by the PDP-11 processor, its memory, and all of its peripherals ” ',\n",
       "              '“UNIBUS Cable BC11A The BC11A is a 120-conductor cable … The 120 signals include all 56 UNIBUS lines plus 64 grounds. Signals and grounds alternate to minimize crosstalk.”',\n",
       "              'In the above figure, we see provision for five priority levels for I/O devices – levels 4, 5, 6, and 7 and also a level called NPR for Non-Processor Request, a high-priority interrupt for access to memory not involving the processor; this is now called DMA.',\n",
       "              'The PDP-11 is an example of a computer with memory-mapped Input/Output. What this name implies is that all devices, both the memory and the I/O devices share a common bus (on the original PDP-11 it was the UNIBUS™). Some addresses on the bus would refer to I/O devices and some to memory. On the early PDP-11 specifications, octal addresses 760 000 through 777 777 referred to I/O devices and all other addresses were memory.',\n",
       "              'A computer with isolated I/O has at least two buses – one for memory references and one for access to the I/O devices. The advantage of the isolated I/O design is that all of the addresses are available for memory references, thus allowing more memory. It is the assembly language instruction that indicates that the address given is to refer to an I/O device. The disadvantage of this scheme is that there must be explicit I/O instructions, leading to a design that is “less elegant”.',\n",
       "              'One should be serious about the design issue of explicit I/O instructions. A typical computer design will allocate a fixed number of bits to specify the instruction to be executed; this limits the number of instructions possible. If N bits are allocated to specify the instruction, there is a maximum of 2 N instructions. For a small value of N, this can become a serious constraint on the design and the elimination of two I/O instructions (Read from Device and Write to Device) becomes an appealing option.',\n",
       "              'Older bus designs and Input/Output bus designs tend to be asynchronous. Such a bus, called an asynchronous bus, operates without a clock to synchronize data transfers and thus must use control signals to coordinate the operations; these must conform to a bus protocol. A typical protocol for an asynchronous bus includes the following signals',\n",
       "              'Bus Request the device is requesting use of the bus to transfer data Bus Grant the device is granted use of the bus Bus Busy asserted by a device granted the bus to prevent collisions',\n",
       "              'A recent (10/20/2004) search the Dell Computer Internet web site ( http://www.dell.com/ ) included a claim that the computer had “next generation dual channel DDR2 memory – up to 16 GB of 400MHz registered ECC memory”. This implies that the main bus connecting the CPU to memory operated at 400 MHZ (with a clock cycle of 2.5 nanoseconds) and could make transfers at a rate of 800 million per second – that is 400 million times 2 as the bus is a second generation Double Data Rate device. It can be inferred from this web site that the memory bus is actually 64-bits wide, allowing for the parallel transfer of eight bytes at a time, giving a data transfer rate of 6,400 million per second or 6.4GB/sec.',\n",
       "              'The reader should note that the figure 6.4GB/sec and the figures leading up to that result are the work of the author of these notes and might not be exactly correct.',\n",
       "              'As with any system, a computer can be viewed from a number of perspectives or levels of detail. Each level corresponds to a virtual machine – one able to execute directly a specific language. For example, many people view computers as virtual machines that directly execute e-mail programs, spread sheets, or word processors. These people do not care about the lower level instructions that actually cause the machine to function, nor should they.',\n",
       "              'Put another way, many people consider the computer as just another appliance – that is, something that does its job with little human interaction. In this author’s opinion, this fact is one of the major achievements of the computer industry.',\n",
       "              'In order to motivate the idea of levels of machines, let us consider what might be called “levels of automobiles” or more precisely, the level of detail at which a particular user chooses to understand an automobile. As an example, let us think about the Rolls Royce Phantom (the 2004 model is priced at only $470,000, in case you want to buy one). There are a number of levels at which to view this automobile.',\n",
       "              'The VIP (very rich person or diplomat) will view the automobile as something that transports him or her to the desired destination. Admittedly, the automobile does not drive itself, but it might as well, given the fact that it almost always has a paid chauffeur.',\n",
       "              'The casual driver will understand the basics of operating the vehicle – use of the keys, transmission, steering wheel, and other controls. To this person, the automobile is just a machine that can be controlled in predictable ways.',\n",
       "              'The more involved driver will, in addition, understand that the automobile comprises a number of subsystems, such as the chassis, engine, transmission, and electronic systems. He or she will understand the necessity of system maintenance without being able to perform it.',\n",
       "              'The automobile mechanic will understand the details of each of the automobile subsystems and how to repair them. Note that this is a lower (more detailed) level of understanding than the involved driver. The mechanic not only understands that the subsystems exist but also understands how they work and how to fix them.',\n",
       "              'The automobile engineer will understand one of the subsystems in detail, for example the detailed kinetics of fuel combustion, metallurgy of the engine block, or dynamics of the electrical system.',\n",
       "              'A traditional view of the computer (see Tanenbaum  or Murdocca & Heuring ) presents between five and eight levels of machines. Here we construct a typical list.',\n",
       "              '9. Application Programs (Appliance level), 8. High Level (Problem Oriented) Languages, 7. Assembly language, 6. Operating system services (such as BIOS calls) 5. Binary machine language, 4. Micro-operation level (usually microprogramming), 3. Functional units (memory, ALU, etc.) 2. Logic gates, including SSI and MSI components. 1. Transistors and wires.',\n",
       "              'We skip over level 9 (application programs) and begin our top-down study with level 8 (High Level Languages). A high-level language programmer may experience the computer as a machine that directly executes the high-level language, such as C++, Visual Basic, COBOL, or FORTRAN. In fact, very few machines are designed for direct execution of high-level languages (there are several LISP machines and a FORTH machine), but one may imagine a virtual machine that does exactly that. In practice, most virtual machines operating at the high-level language level achieve their effect by compiling the program into a form suitable for execution on a lower-level machine. The figure below shows two of the more common ways in which a high-level language virtual machine functions.',\n",
       "              'Before discussing this figure, it is important to understand the differences between level 6 (the Operating System/BIOS level) and level 5 (the Binary Machine Language level). In some aspects, levels 5 and 6 are identical. The major difference is that level 6 may be regarded as providing standard service routines, such as those in the Basic Input-Output System (BIOS). The operating code for both levels 5 and 6 is binary machine code.',\n",
       "              'Some compilers (mainly the older ones) compile directly to assembly language, which is then assembled into calls to level 6 machine language. Some compile directly to level 6 code.',\n",
       "              'At this point, we see an important part of the separation of levels. Consider modern languages, such as C++ and Java. At level 8, all computers that execute such code appear to be almost identical, with slight differences (such as integer sizes) that can be handled by use of macros and other definitions. At level 7, the computers may appear quite different as each computer brand seems to have its own particular assembly language.',\n",
       "              'The transition between levels 6 and 7 (assembly language & O/S services) and level 5 is often achieved by a linking loader. This transition allows programs to be loaded at any free part of memory, rather than at fixed locations (as was the case with some earlier machines). Thus we have two views of machines – the level 6/7 virtual machine in which the program always loads at a fixed location and the level 5 machine in which the program is relocated.',\n",
       "              'The split between levels 5 and 4 reflects the fact that there are a number of ways in which to implement a Central Processing Unit (CPU) to execute a machine language. The two primary methods for machine language execution are hard-wired and microprogrammed. This separation between these two levels allows a company to build a series of computers with widely differing performance levels but with the same assembly/machine language set. For examples, we look to the IBM 360 series and the DEC (Digital Equipment Corporation – no longer in business) PDP–11 series.',\n",
       "              'Here is a quote from an article by C. Gordon Bell in William Stallings . It discusses two different implementations of the IBM 360 family, each with the same assembly language.',\n",
       "              '“The IBM 360, introduced in 1964, was one of the earliest computer families to span a range of price and performance. Along with the 360, IBM introduced the word ‘architecture’ to refer to the various processing characteristics of a machine as seen by the programmer and his programs. In the initial 360 product family, the model 91 exceeded the model 20 in performance by a factor of 300, in memory size by a factor of 512, and in price by a factor of 100.”',\n",
       "              'The next three layers form the basis for the hardware implementation of the computer. As technology improves, we see two trends in implementation at this level: more powerful units for the same price and equally powerful units for a lesser price. One very early example of this was the IBM 709/7090 series, both of which implemented the same machine language and used the same hardwired control design. The difference is that the IBM 709 used vacuum tubes as the basic circuit elements, while the IBM 7090 used transistors.',\n",
       "              'Probably the major revolution in computer design occurred at these low levels with the introduction to the integrated circuit to replace circuits built from discrete transistors. The transition from vacuum tubes to transistors resulted in considerable gains in reliability and reductions in power usage. The transition from transistors to integrated circuits, especially VLSI (Very Large Scale Integration) chips allowed the introduction of the modern micro-computer and all that has gone with it.',\n",
       "              'One of the recent developments in computer architecture is called by the acronym RISC. Under this classification, a design is either RISC or CISC, with the following definitions.',\n",
       "              'The definition of CISC architecture is very simple – it is any design that does not implement RISC architecture. We now define RISC architecture and give some history of its evolution.',\n",
       "              'One should note that while the name “RISC” is of fairly recent origin (dating to the late 1970’s) the concept can be traced to the work of Seymour Cray, then of Control Data Corporation, on the CDC–6600 and related machines. Mr. Cray did not think in terms of a reduced instruction set, but in terms of a very fast computer with a well-defined purpose – to solve complex mathematical simulations. The resulting design supported only two basic data types (integers and real numbers) and had a very simple, but powerful, instruction set. Looking back at the design of this computer, we see that the CDC–6600 could have been called a RISC design.',\n",
       "              'As we shall see just below, the entire RISC vs. CISC evolution is driven by the desire to obtain maximum performance from a computer at a reasonable price. Mr. Cray’s machines maximized performance by limiting the domain of the problems they would solve.',\n",
       "              'The general characteristic of a CISC architecture is the emphasis on doing more with each instruction. This may involve complex instructions and complex addressing modes; for example the MC68020 processor supports 25 addressing modes.',\n",
       "              'The ability to do more with each instruction allows more operations to be compressed into the same program size, something very desirable if memory costs are high. Some historical data will illustrate the memory issue.',\n",
       "              'Another justification for the CISC architectures was the “semantic gap”, the difference between the structure of the assembly language and the structure of the high level languages (COBOL, C++, Visual Basic, FORTRAN, etc.) that we want the computer to support. It was expected that a more complicated instruction set (more complicated assembly language) would more closely resemble the high level language to be supported and thus facilitate the creation of a compiler for the assembly language.',\n",
       "              'One of the first motivations for the RISC architecture came from a careful study of the implications of the semantic gap. Experimental studies conducted in 1971 by Donald Knuth and 1982 by David Patterson showed that nearly 85% of a programs statements were simple assignment, conditional, or procedure calls. None of these required a complicated instruction set. It was further notes that typical compilers translated complex high level language constructs into simpler assembly language statements, not the complicated assembly language instructions that seemed more likely to be used.',\n",
       "              'The results of this study are quoted from an IEEE Tutorial on RISC architecture . This table shows the percentages of program statements that fall into five broad classifications.',\n",
       "              '“There is quite good agreement in the results of this mixture of languages and applications. Assignment statements predominate, suggesting that the simple movement of data is of high importance. There is also a preponderance of conditional statements (If, Loop). These statements are implemented in machine language with some sort of compare and branch instruction. This suggests that the sequence control mechanism is important.”',\n",
       "              'The “bottom line” for the above results can be summarized as follows. 1) As time progresses, more and more programs will be written in a compiled high- level language, with much fewer written directly in assembly language. 2) The compilers for these languages do not make use of the complex instruction sets provided by the architecture in an attempt to close the semantic gap.',\n",
       "              'In 1979, the author of these notes attended a lecture by a senior design engineer from IBM. He was discussing a feature of an architecture that he designed: he had put about 6 months of highly skilled labor into implementing a particular assembly language instruction and then found that it was used less than 1/10,000 of a percent of the time by any compiler.',\n",
       "              'So the “semantic gap” – the desire to provide a robust architecture for support of high-level language programming turned out to lead to a waste of time and resources. Were there any other justifications for the CISC design philosophy?',\n",
       "              'The other motivation for the RISC architecture is that a complex instruction set implies a slower computer. It is not just the fact that the more complex instructions execute more slowly than the simpler instructions. There is also the fact that making a CPU capable of handling more complex instructions causes it to execute simple instructions more slowly.',\n",
       "              'Thus we are facing the facts that the more complex instruction sets are not necessary and that dropping the ability to support them will yield a faster CPU. There are other factors that favor the RISC architecture, specifically the fact that speed-up techniques such as instruction pre-fetching and instruction pipelining are more easily achieved for simple instructions.',\n",
       "              '1) Fixed instruction length, generally one word. This simplifies instruction fetch. 2) Simplified addressing modes. 3) Fewer and simpler instructions in the instruction set. 4) Only load and store instructions access memory; no add memory to register, etc. 5) Let the compiler do it. Use a good compiler to break complex high-level language statements into a number of simple assembly language statements.',\n",
       "              'The philosophy behind the RISC approach is well described in the IEEE tutorial. Here we pick up on a narrative by a design engineer who worked on the IBM 801 project.',\n",
       "              '“About this point, several people, including those who had been working on microprogramming tools, began to rethink the architectural design principles of the 1970’s. In trying to close the ‘semantic gap’, these principles had actually introduced a ’performance gap’. The attempt to bridge the gap with WCS’s  was unsuccessful.”',\n",
       "              '“Register-oriented architectures have significantly lower data memory bandwidth. Lower data memory bandwidth is highly desirable since data access is less predictable than instruction access and can cause more performance problems.”',\n",
       "              'We note that, even at 6.4 GB/second data transfer rates, access to memory is still a bottleneck in modern computer design, so any design that reduces the requirement for memory access (here called reducing the memory bandwidth) would be advantageous.',\n",
       "              '“The load/store nature of these  architectures is very suitable for effective register allocation by the compiler; furthermore, each eliminated memory reference results in saving an entire instruction.”',\n",
       "              'In a traditional fetch-execute machine, the instruction is first fetched from memory and then executed. Very early in CPU design, it was recognized that the fetch unit should be doing something during the time interval for executing the instruction. The logical thing for the fetch unit to do was to fetch the instruction in the next memory location on the chance that it would be the instruction that would be executed next. This process has been shown to improve computer performance significantly. The logic to pre-fetch instructions is facilitated by the RISC design philosophy that all instructions are the same size, so in a machine based on 32-bit words the pre-fetch unit just grabs the next four bytes.',\n",
       "              'Instruction pre-fetching appears rather simple, except in the presence of program jumps, such as occur in the case of conditional branches and the end of program loops. A lot of work has gone into prediction of the next instruction in such cases, where there are two instructions that could be executed next depending on some condition. It may be possible to execute both candidate instructions and discard the result of the instruction not in the true execution path.',\n",
       "              'The complex instructions in a CISC computer tend to require more support in the execution than can conveniently be provided by a hardwired control unit. For this reason, most CISC computers are microprogrammed to handle the complexity of each of the instructions. For this reason, most CISC instructions require a number of system clock cycles to execute. The RISC approach emphasizes use of a simpler instruction set that can easily be supported by a hardwired control unit. As a side effect, most RISC instructions can be executed in one clock cycle. A given computer program will compile into more RISC instructions than CISC instructions, but the CISC instructions execute more slowly than the RISC instructions. The overall effect on the computer program may be hard to predict.',\n",
       "              '“Reducing the instruction set further reduces the work a RISC processor has to do. Since RISC has fewer types of instructions than CISC, a RISC instruction requires less processing logic to interpret than a CISC instruction. The effect of such simplification is to speed up the execution rate for RISC instructions. In a RISC implementation it is theoretically possible to execute an instruction each time the computer’s logic clock ticks. In practice the clock rate of a RISC processor is usually three times that of the instruction rate.”',\n",
       "              'We close this section by giving a comparison of some RISC and CISC computers and quoting some of the experience of the Digital Equipment Corporation when it tried to manufacture a RISC version of its Micro-VAX (a follow-on to the PDP-11).',\n",
       "              'Note the control memory size on the two RISC type computers – each has no control memory. This implies that the control unit is purely hardwired. Experience in the 1980’s and early 1990’s suggested that microprogrammed control units were preferable, even if they were a bit slower than hardwired units. It was argued that the speed of the control unit was not the limiting factor in performance, and it probably was not. The plain fact, however, was that implementing a hardwired control unit for some of the complex instruction sets was a daunting challenge not willingly faced by the computer designers. Rather than spend a great fortune on designing, building, and debugging such a unit, they elected to create control units that could be managed – these were microprogrammed.',\n",
       "              'In considering the RISC design, we should recognize the fact that it is not equivalent to use of a hardwired control unit; only more compatible with such a unit. Many modern control units might be considered as hybrid, with mostly hardwired control and provisions for the use of micro-routines (in microcode) to handle useful, but complex, instructions.',\n",
       "              'It has been hinted above that microprogramming has been used as a tool to allow feasible and cost-effective implementations of complex instruction sets. It is profitable to consider the correlation between complex instructions and the use of a microprogrammed control unit; specifically asking the question of the allocation of lines of microcode to assembly language instructions.',\n",
       "              'Digital Equipment Corporation (DEC) undertook an experiment to investigate this correlation and produced a design yielding interesting, but not surprising, results, which are again quoted from the IEEE tutorial on RISC architecture.',\n",
       "              '“DEC reported a subsetting experiment on two implementations of the VAX architecture in VLSI. The VLSI VAX has nine custom VLSI chips and implements the complete VAX-11 instruction set. DEC found that 20.0 percent of the instructions are responsible for 60.0 percent of the microcode and yet are only 0.2 percent of all instructions executed. By trapping to software to execute these instructions, the MicroVAX 32 was able to fit the subset architecture onto only one chip, with an optional floating-point processor on another chip... The VLSI VAX uses five to ten times the resources of the MicroVAX 32 to implement the full instruction set, yet is only 20 percent faster.”'],\n",
       "             'https://www.tutorialspoint.com/Computer-System-Architecture': ['A computer system is basically a machine that simplifies complicated tasks. It should maximize performance and reduce costs as well as power consumption.The different components in the Computer System Architecture are Input Unit, Output Unit, Storage Unit, Arithmetic Logic Unit, Control Unit etc.',\n",
       "              'The input data travels from input unit to ALU. Similarly, the computed data travels from ALU to output unit. The data constantly moves from storage unit to ALU and back again. This is because stored data is computed on before being stored again. The control unit controls all the other units as well as their data.',\n",
       "              'The input unit provides data to the computer system from the outside. So, basically it links the external environment with the computer. It takes data from the input devices, converts it into machine language and then loads it into the computer system. Keyboard, mouse etc. are the most commonly used input devices.',\n",
       "              'The output unit provides the results of computer process to the users i.e it links the computer with the external environment. Most of the output data is the form of audio or video. The different output devices are monitors, printers, speakers, headphones etc.',\n",
       "              'Storage unit contains many computer components that are used to store data. It is traditionally divided into primary storage and secondary storage.Primary storage is also known as the main memory and is the memory directly accessible by the CPU. Secondary or external storage is not directly accessible by the CPU. The data from secondary storage needs to be brought into the primary storage before the CPU can use it. Secondary storage contains a large amount of data permanently.',\n",
       "              'All the calculations related to the computer system are performed by the arithmetic logic unit. It can perform operations like addition, subtraction, multiplication, division etc. The control unit transfers data from storage unit to arithmetic logic unit when calculations need to be performed. The arithmetic logic unit and the control unit together form the central processing unit.',\n",
       "              'This unit controls all the other units of the computer system and so is known as its central nervous system. It transfers data throughout the computer as required including from storage unit to central processing unit and vice versa. The control unit also dictates how the memory, input output devices, arithmetic logic unit etc. should behave.'],\n",
       "             'https://online.sunderland.ac.uk/what-is-computer-architecture/': ['All computers, no matter their size, are based around a set of rules stating how software and hardware join together and interact to make them work. This is what is known as computer architecture. In this article we’re going to delve into what computer architecture actually is.',\n",
       "              'Computer architecture is the organisation of the components which make up a computer system and the meaning of the operations which guide its function. It defines what is seen on the machine interface, which is targeted by programming languages and their compilers.',\n",
       "              'System design includes all hardware parts of a computer, including data processors, multiprocessors, memory controllers, and direct memory access. It also includes the graphics processing unit (GPU). This part is the physical computer system.',\n",
       "              'This includes the functions and capabilities of the central processing unit (CPU). It is the embedded programming language and defines what programming it can perform or process. This part is the software that makes the computer run, such as operating systems like Windows on a PC or iOS on an Apple iPhone, and includes data formats and the programmed instruction set.',\n",
       "              'Microarchitecture is also known as computer organisation and defines the data processing and storage element and how they should be implemented into the ISA. It is the hardware implementation of how an ISA is implemented in a particular processor.',\n",
       "              'CISC processors have a single processing unit, external memory, and a small register set with hundreds of different instructions. These processors have a single instruction to perform a task, and have the advantage of making the job of the programmer easier, as fewer lines of code are needed to get the job done. This approach uses less memory, but can take longer to complete instructions.',\n",
       "              'The RISC architecture was the result of a rethink, which has led to the development of high-performance processors. The hardware is kept as simple and fast as possible, and complex instructions can be performed with simpler instructions.',\n",
       "              'Microprocessors are digital systems which read and execute machine language instructions. Instructions are represented in a symbolic format called an assembly language. These are processors which are implemented on a single, integrated circuit. Common microprocessors used today are the Intel Pentium series, IBM PowerPC, and the Sun SPARC, among others. Nearly all modern processors are microprocessors, which are often available as standard on von Neumann machines.',\n",
       "              'Mathematician John von Neumann and his colleagues proposed the von Neumann architecture in 1945, which stated that a computer consists of: a processor with an arithmetic and logic unit (ALU) and a control unit; a memory unit that can communicate directly with the processor using connections called buses; connections for input/output devices; and a secondary storage for saving and backing up data.',\n",
       "              'The central computation concept of this architecture is that instructions and data are both loaded into the same memory unit, which is the main memory of the computer and consists of a set of addressable locations. The processor can then access the instructions and data required for the execution of a computer program using dedicated connections called buses – an address bus which is used to identify the addressed location and a data bus which is used to transfer the contents to and from a location.',\n",
       "              'Computers as physical objects have changed dramatically in the 76 years since the von Neumann architecture was proposed. Supercomputers in the 1940s took up a whole room but had very basic functionality, compared to a modern smartwatch which is small in size but has dramatically higher performance. However, at their core, computers have changed very little and almost all of those created between then and now have been run on virtually the same von Neumann architecture.',\n",
       "              'There are a number of reasons why the von Neumann architecture has proven to be so successful. It is relatively easy to implement in hardware, and von Neumann machines are deterministic and introspectable. They can be described mathematically and every step of their computing process is understood. You can also rely on them to always generate the same output on one set of inputs.',\n",
       "              'The biggest challenge with von Neumann machines is that they can be difficult to code. This has led to the growth of computer programming, which takes real-world problems and explains them to von Neumann machines.',\n",
       "              'When a software program is being written, an algorithm is reduced to the formal instructions that a von Neumann machine can follow. However, the challenge is that not all algorithms and problems are easy to reduce, leaving unsolved problems.',\n",
       "              'The Harvard architecture keeps instructions and data in separate memories, and the processor accesses these memories using separate buses. The processor is connected to the ‘instructions memory’ using a dedicated set of address and data buses, and is connected to the ‘data memory’ using a different set of address and data buses.',\n",
       "              'At the University of Sunderland we offer a 100% online learning MSc Computer Science course, designed for individuals who aren’t from a computer science background and want to change their career path, or for those who want to incorporate computer science knowledge into their current field for career progression.',\n",
       "              'Study part-time and entirely online and grow your global network as you connect with peers from all over the world. Apply today and start within weeks, we have six start dates a year.'],\n",
       "             'https://www.britannica.com/science/computer-science/Architecture-and-organization': ['Computer architecture deals with the design of computers, data storage devices, and networking components that store and run programs, transmit data, and drive interactions between computers, across networks, and with users. Computer architects use parallelism and various strategies for memory organization to design computing systems with very high performance. Computer architecture requires strong communication between computer scientists and computer engineers, since they both focus fundamentally on hardware design.',\n",
       "              'Computers also have another level of memory called a cache, a small, extremely fast (compared with the main memory, or random access memory ) unit that can be used to store information that is urgently or frequently needed. Current research includes cache design and algorithms that can predict what data is likely to be needed next and preload it into the cache for improved performance.',\n",
       "              'I/O controllers connect the computer to specific input devices (such as keyboards and touch screen displays) for feeding information to the memory, and output devices (such as printers and displays) for transmitting information from the memory to users. Additional I/O controllers connect the computer to a network via ports that provide the conduit through which data flows when the computer is connected to the Internet.',\n",
       "              'Linked to the I/O controllers are secondary storage devices, such as a disk drive, that are slower and have a larger capacity than main or cache memory. Disk drives are used for maintaining permanent data. They can be either permanently or temporarily attached to the computer in the form of a compact disc (CD), a digital video disc (DVD), or a memory stick (also called a flash drive).',\n",
       "              'The operation of a computer, once a program and some data have been loaded into RAM, takes place as follows. The first instruction is transferred from RAM into the control unit and interpreted by the hardware circuitry. For instance, suppose that the instruction is a string of bits that is the code for LOAD 10. This instruction loads the contents of memory location 10 into the ALU. The next instruction, say ADD 15, is fetched. The control unit then loads the contents of memory location 15 into the ALU and adds it to the number already there. Finally, the instruction STORE 20 would store that sum into location 20. At this level, the operation of a computer is not much different from that of a pocket calculator.',\n",
       "              'In general, programs are not just lengthy sequences of LOAD, STORE, and arithmetic operations. Most importantly, computer languages include conditional instructions—essentially, rules that say, “If memory location n satisfies condition a, do instruction number x next, otherwise do instruction y.” This allows the course of a program to be determined by the results of previous operations—a critically important ability.',\n",
       "              'Finally, programs typically contain sequences of instructions that are repeated a number of times until a predetermined condition becomes true. Such a sequence is called a loop. For example, a loop would be needed to compute the sum of the first n integers, where n is a value stored in a separate memory location. Computer architectures that can execute sequences of instructions, conditional instructions, and loops are called “ Turing complete,” which means that they can carry out the execution of any algorithm that can be defined. Turing completeness is a fundamental and essential characteristic of any computer organization.',\n",
       "              'An important area related to architecture is the design of microprocessors, which are complete CPUs—control unit, ALU, and memory—on a single integrated circuit chip. Additional memory and I/O control circuitry are linked to this chip to form a complete computer. These thumbnail-sized devices contain millions of transistors that implement the processing and memory units of modern computers.',\n",
       "              'VLSI microprocessor design occurs in a number of stages, which include creating the initial functional or behavioral specification, encoding this specification into a hardware description language, and breaking down the design into modules and generating sizes and shapes for the eventual chip components. It also involves chip planning, which includes building a “floor plan” to indicate where on the chip each component should be placed and connected to other components. Computer scientists are also involved in creating the computer-aided design (CAD) tools that support engineers in the various stages of chip design and in developing the necessary theoretical results, such as how to efficiently design a floor plan with near-minimal area that satisfies the given constraints.',\n",
       "              'Advances in integrated circuit technology have been incredible. For example, in 1971 the first microprocessor chip (Intel Corporation ’s 4004) had only 2,300 transistors, in 1993 Intel’s Pentium chip had more than 3 million transistors, and by 2000 the number of transistors on such a chip was about 50 million. The Power7 chip introduced in 2010 by IBM contained approximately 1 billion transistors. The phenomenon of the number of transistors in an integrated circuit doubling about every two years is widely known as Moore’s law.',\n",
       "              'Computational science applies computer simulation, scientific visualization, mathematical modeling, algorithms, data structures, networking, database design, symbolic computation, and high-performance computing to help advance the goals of various disciplines. These disciplines include biology, chemistry, fluid dynamics, archaeology, finance, sociology, and forensics. Computational science has evolved rapidly, especially because of the dramatic growth in the volume of data transmitted from scientific instruments. This phenomenon has been called the “big data” problem.',\n",
       "              'The requirements of big-data scientific problems, including the solution of ever larger systems of equations, engage the use of large and powerful arrays of processors (called multiprocessors or supercomputers) that allow many calculations to proceed in parallel by assigning them to separate processing elements. These activities have sparked much interest in parallel computer architecture and algorithms that can be carried out efficiently on such machines.',\n",
       "              'Graphics and visual computing is the field that deals with the display and control of images on a computer screen. This field encompasses the efficient implementation of four interrelated computational tasks: rendering, modeling, animation, and visualization. Graphics techniques incorporate principles of linear algebra, numerical integration, computational geometry, special-purpose hardware, file formats, and graphical user interfaces (GUIs) to accomplish these complex tasks.',\n",
       "              'A challenge for computer graphics is the development of efficient algorithms that manipulate the myriad of lines, triangles, and polygons that make up a computer image. In order for realistic on-screen images to be presented, each object must be rendered as a set of planar units. Edges must be smoothed and textured so that their underlying construction from polygons is not obvious to the naked eye. In many applications, still pictures are inadequate, and rapid display of real-time images is required. Both extremely efficient algorithms and state-of-the-art hardware are needed to accomplish real-time animation. (For more technical details of graphics displays, see computer graphics.)',\n",
       "              'Thus, the field of HCI emerged to model, develop, and measure the effectiveness of various types of interfaces between a computer application and the person accessing its services. GUIs enable users to communicate with the computer by such simple means as pointing to an icon with a mouse or touching it with a stylus or forefinger. This technology also supports windowing environments on a computer screen, which allow users to work with different applications simultaneously, one in each window.'],\n",
       "             'https://people.cs.ksu.edu/~schmidt/300s05/Lectures/ArchNotes/arch.html': ['When the instruction arrives, copy it from the data buffer into the instruction register. Increment the instruction counter to 4 (that is, 0000 0100). Decode: Read the first (leading or high-order) bits and see that they indicate an ADD. Extract the bits that state the two registers to be added, here, R2 and R1. Execute: Signal the ALU to add the values in registers 1 and 2 and place the result in register 2.',\n",
       "              'Increment the instruction counter to 4 (that is, 0000 0100). Decode: Read the first (leading or high-order) bits and see that they indicate an ADD. Extract the bits that state the two registers to be added, here, R2 and R1. Execute: Signal the ALU to add the values in registers 1 and 2 and place the result in register 2.',\n",
       "              \"Here is a revised picture of the computer's storage, which shows the inclusion of the operating system (``OS'') and the division of the remaining storage for the multiple user programs that are executing: The actions of the operating system are developed in a later lecture.\"],\n",
       "             'https://m-cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext': ['We began our Turing Lecture June 4, 2018 11 with a review of computer architecture since the 1960s. In addition to that review, here, we highlight current challenges and identify future opportunities, projecting another golden age for the field of computer architecture in the next decade, much like the 1980s when we did the research that led to our award, delivering gains in cost, energy, and security, as well as performance.',\n",
       "              'Software talks to hardware through a vocabulary called an instruction set architecture (ISA). By the early 1960s, IBM had four incompatible lines of computers, each with its own ISA, software stack, I/O system, and market nichetargeting small business, large business, scientific, and real time, respectively. IBM engineers, including ACM A.M. Turing Award laureate Fred Brooks, Jr., thought they could create a single ISA that would efficiently unify all four of these ISA bases.',\n",
       "              'The table here lists four models of the new System/360 ISA IBM announced April 7, 1964. The data paths vary by a factor of 8, memory capacity by a factor of 16, clock rate by nearly 4, performance by 50, and cost by nearly 6. The most expensive computers had the widest control stores because more complicated data paths used more control lines. The least-costly computers had narrower control stores due to simpler hardware but needed more microinstructions since they took more clock cycles to execute a System/360 instruction.',\n",
       "              'Facilitated by microprogramming, IBM bet the future of the company that the new ISA would revolutionize the computing industry and won the bet. IBM dominated its markets, and IBM mainframe descendants of the computer family announced 55 years ago still bring in $10 billion in revenue per year.',\n",
       "              'As seen repeatedly, although the marketplace is an imperfect judge of technological issues, given the close ties between architecture and commercial computers, it eventually determines the success of architecture innovations that often require significant engineering investment.',\n",
       "              \"Integrated circuits, CISC, 432, 8086, IBM PC. When computers began using integrated circuits, Moore's Law meant control stores could become much larger. Larger memories in turn allowed much more complicated ISAs. Consider that the control store of the VAX-11/780 from Digital Equipment Corp. in 1977 was 5,120 words x 96 bits, while its predecessor used only 256 words x 56 bits.\",\n",
       "              'Some manufacturers chose to make microprogramming available by letting select customers add custom features they called \"writable control store\" (WCS). The most famous WCS computer was the Alto 36 Turing laureates Chuck Thacker and Butler Lampson, together with their colleagues, created for the Xerox Palo Alto Research Center in 1973. It was indeed the first personal computer, sporting the first bit-mapped display and first Ethernet local-area network. The device controllers for the novel display and network were microprograms stored in a 4,096-word x 32-bit WCS.',\n",
       "              'Microprocessors were still in the 8-bit era in the 1970s (such as the Intel 8080) and programmed primarily in assembly language. Rival designers would add novel instructions to outdo one another, showing their advantages through assembly language examples.',\n",
       "              \"Gordon Moore believed Intel's next ISA would last the lifetime of Intel, so he hired many clever computer science Ph.D.'s and sent them to a new facility in Portland to invent the next great ISA. The 8800, as Intel originally named it, was an ambitious computer architecture project for any era, certainly the most aggressive of the 1980s. It had 32-bit capability-based addressing, object-oriented architecture, variable-bit-length instructions, and its own operating system written in the then-new programming language Ada.\",\n",
       "              'This ambitious project was alas several years late, forcing Intel to start an emergency replacement effort in Santa Clara to deliver a 16-bit microprocessor in 1979. Intel gave the new team 52 weeks to develop the new \"8086\" ISA and design and build the chip. Given the tight schedule, designing the ISA took only 10 person-weeks over three regular calendar weeks, essentially by extending the 8-bit registers and instruction set of the 8080 to 16 bits. The team completed the 8086 on schedule but to little fanfare when announced.',\n",
       "              \"To Intel's great fortune, IBM was developing a personal computer to compete with the Apple II and needed a 16-bit microprocessor. IBM was interested in the Motorola 68000, which had an ISA similar to the IBM 360, but it was behind IBM's aggressive schedule. IBM switched instead to an 8-bit bus version of the 8086. When IBM announced the PC on August 12, 1981, the hope was to sell 250,000 PCs by 1986. The company instead sold 100 million worldwide, bestowing a very bright future on the emergency replacement Intel ISA.\",\n",
       "              \"Intel's original 8800 project was renamed iAPX-432 and finally announced in 1981, but it required several chips and had severe performance problems. It was discontinued in 1986, the year after Intel extended the 16-bit 8086 ISA in the 80386 by expanding its registers from 16 bits to 32 bits. Moore's prediction was thus correct that the next ISA would last as long as Intel did, but the marketplace chose the emergency replacement 8086 rather than the anointed 432. As the architects of the Motorola 68000 and iAPX-432 both learned, the marketplace is rarely patient.\",\n",
       "              'From complex to reduced instruction set computers. The early 1980s saw several investigations into complex instruction set computers (CISC) enabled by the big microprograms in the larger control stores. With Unix demonstrating that even operating systems could use high-level languages, the critical question became: \"What instructions would compilers generate?\" instead of \"What assembly language would programmers use?\" Significantly raising the hardware/software interface created an opportunity for architecture innovation.',\n",
       "              \"In today's post-PC era, x86 shipments have fallen almost 10% per year since the peak in 2011, while chips with RISC processors have skyrocketed to 20 billion.\",\n",
       "              'For example, Figure 1 shows the RISC-I 8 and MIPS 12 microprocessors developed at the University of California, Berkeley, and Stanford University in 1982 and 1983, respectively, that demonstrated the benefits of RISC. These chips were eventually presented at the leading circuit conference, the IEEE International Solid-State Circuits Conference, in 1984. 33, 35 It was a remarkable moment when a few graduate students at Berkeley and Stanford could build microprocessors that were arguably superior to what industry could build.',\n",
       "              'These academic chips inspired many companies to build RISC microprocessors, which were the fastest for the next 15 years. The explanation is due to the following formula for processor performance:',\n",
       "              'DEC engineers later showed 2 that the more complicated CISC ISA executed about 75% of the number instructions per program as RISC (the first term), but in a similar technology CISC executed about five to six more clock cycles per instruction (the second term), making RISC microprocessors approximately 4x faster.',\n",
       "              \"Such formulas were not part of computer architecture books in the 1980s, leading us to write Computer Architecture: A Quantitative Approach 13 in 1989. The subtitle suggested the theme of the book: Use measurements and benchmarks to evaluate trade-offs quantitatively instead of relying more on the architect's intuition and experience, as in the past. The quantitative approach we used was also inspired by what Turing laureate Donald Knuth's book had done for algorithms. 20\",\n",
       "              'VLIW, EPIC, Itanium. The next ISA innovation was supposed to succeed both RISC and CISC. Very long instruction word (VLIW) 7 and its cousin, the explicitly parallel instruction computer (EPIC), the name Intel and Hewlett Packard gave to the approach, used wide instructions with multiple independent operations bundled together in each instruction. VLIW and EPIC advocates at the time believed if a single instruction could specify, say, six independent operationstwo data transfers, two integer operations, and two floating point operationsand compiler technology could efficiently assign operations into the six instruction slots, the hardware could be made simpler. Like the RISC approach, VLIW and EPIC shifted work from the hardware to the compiler.',\n",
       "              'AMD and Intel used 500-person design teams and superior semiconductor technology to close the performance gap between x86 and RISC. Again inspired by the performance advantages of pipelining simple vs. complex instructions, the instruction decoder translated the complex x86 instructions into internal RISC-like microinstructions on the fly. AMD and Intel then pipelined the execution of the RISC microinstructions. Any ideas RISC designers were using for performanceseparate instruction and data caches, second-level caches on chip, deep pipelines, and fetching and executing several instructions simultaneouslycould then be incorporated into the x86. AMD and Intel shipped roughly 350 million x86 microprocessors annually at the peak of the PC era in 2011. The high volumes and low margins of the PC industry also meant lower prices than RISC computers.',\n",
       "              'Given the hundreds of millions of PCs sold worldwide each year, PC software became a giant market. Whereas software providers for the Unix marketplace would offer different software versions for the different commercial RISC ISAsAlpha, HP-PA, MIPS, Power, and SPARCthe PC market enjoyed a single ISA, so software developers shipped \"shrink wrap\" software that was binary compatible with only the x86 ISA. A much larger software base, similar performance, and lower prices led the x86 to dominate both desktop computers and small-server markets by 2000.',\n",
       "              \"Apple helped launch the post-PC era with the iPhone in 2007. Instead of buying microprocessors, smartphone companies built their own systems on a chip (SoC) using designs from other companies, including RISC processors from ARM. Mobile-device designers valued die area and energy efficiency as much as performance, disadvantaging CISC ISAs. Moreover, arrival of the Internet of Things vastly increased both the number of processors and the required trade-offs in die size, power, cost, and performance. This trend increased the importance of design time and cost, further disadvantaging CISC processors. In today's post-PC era, x86 shipments have fallen almost 10% per year since the peak in 2011, while chips with RISC processors have skyrocketed to 20 billion. Today, 99% of 32-bit and 64-bit processors are RISC.\",\n",
       "              'Concluding this historical review, we can say the marketplace settled the RISC-CISC debate; CISC won the later stages of the PC era, but RISC is winning the post-PC era. There have been no new CISC ISAs in decades. To our surprise, the consensus on the best ISA principles for general-purpose processors today is still RISC, 35 years after their introduction.',\n",
       "              '\"If a problem has no solution, it may not be a problem, but a factnot to be solved, but to be coped with over time.\" Shimon Peres',\n",
       "              \"Although Moore's Law held for many decades (see Figure 2), it began to slow sometime around 2000 and by 2018 showed a roughly 15-fold gap between Moore's prediction and current capability, an observation Moore made in 2003 that was inevitable. 27 The current expectation is that the gap will continue to grow as CMOS technology approaches fundamental limits.\",\n",
       "              'Accompanying Moore\\'s Law was a projection made by Robert Dennard called \"Dennard scaling,\" 5 stating that as transistor density increased, power consumption per transistor would drop, so the power per mm 2 of silicon would be near constant. Since the computational capability of a mm 2 of silicon was increasing with each new generation of technology, computers would become more energy efficient. Dennard scaling began to slow significantly in 2007 and faded to almost nothing by 2012 (see Figure 3).',\n",
       "              'Between 1986 and about 2002, the exploitation of instruction level parallelism (ILP) was the primary architectural method for gaining performance and, along with improvements in speed of transistors, led to an annual performance increase of approximately 50%. The end of Dennard scaling meant architects had to find more efficient ways to exploit parallelism.',\n",
       "              'To see how challenging such a design is, consider the difficulty of correctly predicting the outcome of 15 branches. If a processor architect wants to limit wasted work to only 10% of the time, the processor must predict each branch correctly 99.3% of the time. Few general-purpose programs have branches that can be predicted so accurately.',\n",
       "              'To appreciate how this wasted work adds up, consider the data in Figure 4, showing the fraction of instructions that are effectively executed but turn out to be wasted because the processor speculated incorrectly. On average, 19% of the instructions are wasted for these benchmarks on an Intel Core i7. The amount of wasted energy is greater, however, since the processor must use additional energy to restore the state when it speculates incorrectly. Measurements like these led many to conclude architects needed a different approach to achieve performance improvements. The multicore era was thus born.',\n",
       "              \"Real programs have more complex structures of course, with portions that allow varying numbers of processors to be used at any given moment in time. Nonetheless, the need to communicate and synchronize periodically means most applications have some portions that can effectively use only a fraction of the processors. Although Amdahl's Law is more than 50 years old, it remains a difficult hurdle.\",\n",
       "              \"An era without Dennard scaling, along with reduced Moore's Law and Amdahl's Law in full effect means inefficiency limits improvement in performance to only a few percent per year (see Figure 6). Achieving higher rates of performance improvementas was seen in the 1980s and 1990swill require new architectural approaches that use the integrated-circuit capability much more efficiently. We will return to what approaches might work after discussing another major shortcoming of modern computerstheir support, or lack thereof, for computer security.\",\n",
       "              \"Inherent inefficiencies in general-purpose processors, whether from ILP techniques or multicore, combined with the end of Dennard scaling and Moore's Law, make it highly unlikely, in our view, that processor architects and designers can sustain significant rates of performance improvements in general-purpose processors. Given the importance of improving performance to enable new software capabilities, we must ask: What other approaches might be promising?\",\n",
       "              'An interesting research direction concerns whether some of the performance gap can be closed with new compiler technology, possibly assisted by architectural enhancements. Although the challenges in efficiently translating and implementing high-level scripting languages like Python are difficult, the potential gain is enormous. Achieving even 25% of the potential gain could result in Python programs running tens to hundreds of times faster. This simple example illustrates how great the gap is between modern languages emphasizing programmer productivity and traditional approaches emphasizing performance.',\n",
       "              'Second, DSAs can make more effective use of the memory hierarchy. Memory accesses have become much more costly than arithmetic computations, as noted by Horowitz. 16 For example, accessing a block in a 32-kilobyte cache involves an energy cost approximately 200x higher than a 32-bit integer add. This enormous differential makes optimizing memory accesses critical to achieving high-energy efficiency. General-purpose processors run code in which memory accesses typically exhibit spatial and temporal locality but are otherwise not very predictable at compile time. CPUs thus use multilevel caches to increase bandwidth and hide the latency in relatively slow, off-chip DRAMs. These multilevel caches often consume approximately half the energy of the processor but avoid almost all accesses to the off-chip DRAMs that require approximately 10x the energy of a last-level cache access.',\n",
       "              'When caches work well. When caches work well, the locality is very high, meaning, by definition, most of the cache is idle most of the time.',\n",
       "              'In applications where the memory-access patterns are well defined and discoverable at compile time, which is true of typical DSLs, programmers and compilers can optimize the use of the memory better than can dynamically allocated caches. DSAs thus usually use a hierarchy of memories with movement controlled explicitly by the software, similar to how vector processors operate. For suitable applications, user-controlled memories can use much less energy than caches.',\n",
       "              'Third, DSAs can use less precision when it is adequate. General-purpose CPUs usually support 32- and 64-bit integer and floating-point (FP) data. For many applications in machine learning and graphics, this is more accuracy than is needed. For example, in deep neural networks (DNNs), inference regularly uses 4-, 8-, or 16-bit integers, improving both data and computational throughput. Likewise, for DNN training applications, FP is useful, but 32 bits is enough and 16 bits often works.',\n",
       "              'Finally, DSAs benefit from targeting programs written in domain-specific languages (DSLs) that expose more parallelism, improve the structure and representation of memory access, and make it easier to map the application efficiently to a domain-specific processor.',\n",
       "              'DSAs require targeting of high-level operations to the architecture, but trying to extract such structure and information from a general-purpose language like Python, Java, C, or Fortran is simply too difficult. Domain specific languages (DSLs) enable this process and make it possible to program DSAs efficiently. For example, DSLs can make vector, dense matrix, and sparse matrix operations explicit, enabling the DSL compiler to map the operations to the processor efficiently. Examples of DSLs include Matlab, a language for operating on matrices, TensorFlow, a dataflow language used for programming DNNs, P4, a language for programming SDNs, and Halide, a language for image processing specifying high-level transformations.',\n",
       "              'The challenge when using DSLs is how to retain enough architecture independence that software written in a DSL can be ported to different architectures while also achieving high efficiency in mapping the software to the underlying DSA. For example, the XLA system translates Tensorflow to heterogeneous processors that use Nvidia GPUs or Tensor Processor Units (TPUs). 40 Balancing portability among DSAs along with efficiency is an interesting research challenge for language designers, compiler creators, and DSA architects.',\n",
       "              'Example DSA TPU v1. As an example DSA, consider the Google TPU v1, which was designed to accelerate neural net inference. 17, 18 The TPU has been in production since 2015 and powers applications ranging from search queries to language translation to image recognition to AlphaGo and AlphaZero, the DeepMind programs for playing Go and Chess. The goal was to improve the performance and energy efficiency of deep neural net inference by a factor of 10.',\n",
       "              'CPUs. Intel offers CPUs with many cores enhanced by large multi-level caches and one-dimensional SIMD instructions, the kind of FPGAs used by Microsoft, and a new neural network processor that is closer to a TPU than to a CPU. 19',\n",
       "              'In addition to these large players, dozens of startups are pursuing their own proposals. 25 To meet growing demand, architects are interconnecting hundreds to thousands of such chips to form neural-network supercomputers.',\n",
       "              'This avalanche of DNN architectures makes for interesting times in computer architecture. It is difficult to predict in 2019 which (or even if any) of these many directions will win, but the marketplace will surely settle the competition just as it settled the architectural debates of the past.',\n",
       "              'Inspired by the success of open source software, the second opportunity in computer architecture is open ISAs. To create a \"Linux for processors\" the field needs industry-standard open ISAs so the community can create open source cores, in addition to individual companies owning proprietary ones. If many organizations design processors using the same ISA, the greater competition may drive even quicker innovation. The goal is to provide processors for chips that cost from a few cents to $100.',\n",
       "              'The first example is RISC-V (called \"RISC Five\"), the fifth RISC architecture developed at the University of California, Berkeley. 32 RISC-V\\'s has a community that maintains the architecture under the stewardship of the RISC-V Foundation (http://riscv.org/). Being open allows the ISA evolution to occur in public, with hardware and software experts collaborating before decisions are finalized. An added benefit of an open foundation is the ISA is unlikely to expand primarily for marketing reasons, sometimes the only explanation for extensions of proprietary instruction sets.',\n",
       "              'RISC-V is a modular instruction set. A small base of instructions run the full open source software stack, followed by optional standard extensions designers can include or omit depending on their needs. This base includes 32-bit address and 64-bit address versions. RISC-V can grow only through optional extensions; the software stack still runs fine even if architects do not embrace new extensions. Proprietary architectures generally require upward binary compatibility, meaning when a processor company adds new feature, all future processors must also include it. Not so for RISC-V, whereby all enhancements are optional and can be deleted if not needed by an application. Here are the standard extensions so far, using initials that stand for their full names:',\n",
       "              'A third distinguishing feature of RISC-V is the simplicity of the ISA. While not readily quantifiable, here are two comparisons to the ARMv8 architecture, as developed by the ARM company contemporaneously:',\n",
       "              'Fewer instructions. RISC-V has many fewer instructions. There are 50 in the base that are surprisingly similar in number and nature to the original RISC-I. 30 The remaining standard extensionsM, A, F, and Dadd 53 instructions, plus C added another 34, totaling 137. ARMv8 has more than 500; and',\n",
       "              'Simplicity reduces the effort to both design processors and verify hardware correctness. As the RISC-V targets range from data-center chips to IoT devices, design verification can be a significant part of the cost of development.',\n",
       "              'Fourth, RISC-V is a clean-slate design, starting 25 years later, letting its architects learn from mistakes of its predecessors. Unlike first-generation RISC architectures, it avoids microarchitecture or technology-dependent features (such as delayed branches and delayed loads) or innovations (such as register windows) that were superseded by advances in compiler technology.',\n",
       "              'Beyond RISC-V, Nvidia also announced (in 2017) a free and open architecture 29 it calls Nvidia Deep Learning Accelerator (NVDLA), a scalable, configurable DSA for machine-learning inference. Configuration options include data type (int8, int16, or fp16 ) and the size of the two-dimensional multiply matrix. Die size scales from 0.5 mm 2 to 3 mm 2 and power from 20 milliWatts to 300 milliWatts. The ISA, software stack, and implementation are all open.',\n",
       "              'The Manifesto for Agile Software Development (2001) by Beck et al. 1 revolutionized software development, overcoming the frequent failure of the traditional elaborate planning and documentation in waterfall development. Small programming teams quickly developed working-but-incomplete prototypes and got customer feedback before starting the next iteration. The scrum version of agile development assembles teams of five to 10 programmers doing sprints of two to four weeks per iteration.',\n",
       "              'Once again inspired by a software success, the third opportunity is agile hardware development. The good news for architects is that modern electronic computer aided design (ECAD) tools raise the level of abstraction, enabling agile development, and this higher level of abstraction increases reuse across designs.',\n",
       "              'For research purposes, we could stop at tape in, as area, energy, and performance estimates are highly accurate. However, it would be like running a long race and stopping 100 yards before the finish line because the runner can accurately predict the final time. Despite all the hard work in race preparation, the runner would miss the thrill and satisfaction of actually crossing the finish line. One advantage hardware engineers have over software engineers is they build physical things. Getting chips back to measure, run real programs, and show to their friends and family is a great joy of hardware design.',\n",
       "              'Many researchers assume they must stop short because fabricating chips is unaffordable. When designs are small, they are surprisingly inexpensive. Architects can order 100 1-mm 2 chips for only $14,000. In 28 nm, 1 mm 2 holds millions of transistors, enough area for both a RISC-V processor and an NVLDA accelerator. The outermost level is expensive if the designer aims to build a large chip, but an architect can demonstrate many novel ideas with small chips.',\n",
       "              'To benefit from the lessons of history, architects must appreciate that software innovations can also inspire architects, that raising the abstraction level of the hardware/software interface yields opportunities for innovation, and that the marketplace ultimately settles computer architecture debates. The iAPX-432 and Itanium illustrate how architecture investment can exceed returns, while the S/360, 8086, and ARM deliver high annual returns lasting decades with no end in sight.',\n",
       "              \"The end of Dennard scaling and Moore's Law and the deceleration of performance gains for standard microprocessors are not problems that must be solved but facts that, recognized, offer breathtaking opportunities. High-level, domain-specific languages and architectures, freeing architects from the chains of proprietary instruction sets, along with demand from the public for improved security, will usher in a new golden age for computer architects. Aided by open source ecosystems, agilely developed chips will convincingly demonstrate advances and thereby accelerate commercial adoption. The ISA philosophy of the general-purpose processors in these chips will likely be RISC, which has stood the test of time. Expect the same rapid improvement as in the last golden age, but this time in terms of cost, energy, and security, as well as in performance.\",\n",
       "              '1. Beck, K., Beedle, M., Van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M. ... and Kern, J. Manifesto for Agile Software Development, 2001; https://agilemanifesto.org/',\n",
       "              '2. Bhandarkar, D. and Clark, D.W. Performance from architecture: Comparing a RISC and a CISC with similar hardware organization. In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (Santa Clara, CA, Apr. 811). ACM Press, New York, 1991, 310319.',\n",
       "              '4. Dally, W. et al. Hardware-enabled artificial intelligence. In Proceedings of the Symposia on VLSI Technology and Circuits (Honolulu, HI, June 1822). IEEE Press, 2018, 36.',\n",
       "              '5. Dennard, R. et al. Design of ion-implanted MOSFETs with very small physical dimensions. IEEE Journal of Solid State Circuits 9, 5 (Oct. 1974), 256268.',\n",
       "              '6. Emer, J. and Clark, D. A characterization of processor performance in the VAX-11/780. In Proceedings of the 11 th International Symposium on Computer Architecture (Ann Arbor, MI, June). ACM Press, New York, 1984, 301310.',\n",
       "              '8. Fitzpatrick, D.T., Foderaro, J.K., Katevenis, M.G., Landman, H.A., Patterson, D.A., Peek, J.B., Peshkess, Z., Séquin, C.H., Sherburne, R.W., and Van Dyke, K.S. A RISCy approach to VLSI. ACM SIGARCH Computer Architecture News 10, 1 (Jan. 1982), 2832.',\n",
       "              '10. Fowers, J. et al. A configurable cloud-scale DNN processor for real-time AI. In Proceedings of the 45 th ACM/IEEE Annual International Symposium on Computer Architecture (Los Angeles, CA, June 26). IEEE, 2018, 114.',\n",
       "              '11. Hennessy, J. and Patterson, D. A New Golden Age for Computer Architecture. Turing Lecture delivered at the 45 th ACM/IEEE Annual International Symposium on Computer Architecture (Los Angeles, CA, June 4, 2018); http://iscaconf.org/isca2018/turing_lecture.html; https://www.youtube.com/watch?v=3LVeEjsn8Ts',\n",
       "              '12. Hennessy, J., Jouppi, N., Przybylski, S., Rowen, C., Gross, T., Baskett, F., and Gill, J. MIPS: A microprocessor architecture. ACM SIGMICRO Newsletter 13, 4 (Oct. 5, 1982), 1722.',\n",
       "              '14. Hill, M. A primer on the meltdown and Spectre hardware security design flaws and their important implications, Computer Architecture Today blog (Feb. 15, 2018); https://www.sigarch.org/a-primer-on-the-meltdown-spectre-hardware-security-design-flaws-and-their-important-implications/',\n",
       "              '15. Hopkins, M. A critical look at IA-64: Massive resources, massive ILP, but can it deliver? Microprocessor Report 14, 2 (Feb. 7, 2000), 15.',\n",
       "              \"16. Horowitz M. Computing's energy problem (and what we can do about it). In Proceedings of the IEEE International Solid-State Circuits Conference Digest of Technical Papers (San Francisco, CA, Feb. 913). IEEE Press, 2014, 1014.\",\n",
       "              '17. Jouppi, N., Young, C., Patil, N., and Patterson, D. A domain-specific architecture for deep neural networks. Commun. ACM 61, 9 (Sept. 2018), 5058.',\n",
       "              '18. Jouppi, N.P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., and Boyle, R. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44 th ACM/IEEE Annual International Symposium on Computer Architecture (Toronto, ON, Canada, June 2428). IEEE Computer Society, 2017, 112.',\n",
       "              '22. Kung, H. and Leiserson, C. Systolic arrays (for VLSI). Chapter in Sparse Matrix Proceedings Vol. 1. Society for Industrial and Applied Mathematics, Philadelphia, PA, 1979, 256282.',\n",
       "              '23. Lee, Y., Waterman, A., Cook, H., Zimmer, B., Keller, B., Puggelli, A. ... and Chiu, P. An agile approach to building RISC-V microprocessors. IEEE Micro 36, 2 (Feb. 2016), 820.',\n",
       "              '25. Metz, C. Big bets on A.I. open a new frontier for chip start-ups, too. The New York Times (Jan. 14, 2018).',\n",
       "              \"27. Moore, G. No exponential is forever: But 'forever' can be delayed! . In Proceedings of the IEEE International Solid-State Circuits Conference Digest of Technical Papers (San Francisco, CA, Feb. 13). IEEE, 2003, 2023.\",\n",
       "              '28. Moore, G. Progress in digital integrated electronics. In Proceedings of the International Electronic Devices Meeting (Washington, D.C., Dec.). IEEE, New York, 1975, 1113.',\n",
       "              '33. Rowen, C., Przbylski, S., Jouppi, N., Gross, T., Shott, J., and Hennessy, J. A pipelined 32b NMOS microprocessor. In Proceedings of the IEEE International Solid-State Circuits Conference Digest of Technical Papers (San Francisco, CA, Feb. 2224) IEEE, 1984, 180181.',\n",
       "              '34. Schwarz, M., Schwarzl, M., Lipp, M., and Gruss, D. Netspectre: Read arbitrary memory over network. arXiv preprint, 2018; https://arxiv.org/pdf/1807.10535.pdf',\n",
       "              '35. Sherburne, R., Katevenis, M., Patterson, D., and Sequin, C. A 32b NMOS microprocessor with a large register file. In Proceedings of the IEEE International Solid-State Circuits Conference (San Francisco, CA, Feb. 2224). IEEE Press, 1984, 168169.',\n",
       "              '36. Thacker, C., MacCreight, E., and Lampson, B. Alto: A Personal Computer. CSL-79-11, Xerox Palo Alto Research Center, Palo Alto, CA, Aug. 7,1979; http://people.scs.carleton.ca/~soma/distos/fall2008/alto.pdf',\n",
       "              \"37. Turner, P., Parseghian, P., and Linton, M. Protecting against the new 'L1TF' speculative vulnerabilities. Google blog, Aug. 14, 2018; https://cloud.google.com/blog/products/gcp/protectingagainst-the-new-l1tf-speculative-vulnerabilities\",\n",
       "              '38. Van Bulck, J. et al. Foreshadow: Extracting the keys to the Intel SGX kingdom with transient out-of-order execution. In Proceedings of the 27 th USENIX Security Symposium (Baltimore, MD, Aug. 1517). USENIX Association, Berkeley, CA, 2018.',\n",
       "              '39. Wilkes, M. and Stringer, J. Micro-programming and the design of the control circuits in an electronic digital computer. Mathematical Proceedings of the Cambridge Philosophical Society 49, 2 (Apr. 1953), 230238.',\n",
       "              'John L. Hennessy (hennnessy@stanford.edu) is Past-President of Stanford University, Stanford, CA, USA, and is Chairman of Alphabet Inc., Mountain View, CA, USA.',\n",
       "              'David A. Patterson (pattrsn@berkeley.edu) is the Pardee Professor of Computer Science, Emeritus at the University of California, Berkeley, CA, USA, and a Distinguished Engineer at Google, Mountain View, CA, USA.',\n",
       "              'Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and full citation on the first page. Copyright for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or fee. Request permission to publish from permissions@acm.org or fax (212) 869-0481.'],\n",
       "             'http://homepage.cs.uiowa.edu/~jones/arch/risc/': ['Sidenote: Saul Dinman explained in 2003 that GRI was originally General Research Corporation, a private Massachussetts company; when it went public, it had name conflicts with a pre-existing General Research, and then with General Radio as it hunted for a non-conflicting name. Under the name GRI, the company was eventually acquired by venture capitalists in North Carolina, and then by a display manufacturer that wanted to buy their OEM supplier, and finally by Analog Devices. The GRI processor architecture was one of the first bus-oriented architectures built using a printed-circuit backplane. Thousands of GRI-909 systems were sold on an OEM basis, mostly in the industrial control sector. Had marketing and capitalization worked out differently, the GRI-909 might have been an effective competitor for the Data General Nova, another DEC spinoff.',\n",
       "              'With this mechanism, all we need to do is pre-load several pages of memory with tables. For example, a table of sums of the two 4-bit nibbles making up the table address, and a table that maps 8-bit values to the corresponding values with the two 4-bit nibbles swapped. Given these two tables, adding two 4-bit numbers from memory and returning their 4-bit sum to one memory location and the carry out to another location takes only 13 instructions. Clearly, an ALU leads to higher performance, but this alternative to an ALU takes only 16 flipflops, a 12-input and gate to decode the address, and 8 and gates to control which flipflops are read and written. That is far simpler than an adder.',\n",
       "              'Clearly, an ALU leads to higher performance, but this alternative to an ALU takes only 16 flipflops, a 12-input and gate to decode the address, and 8 and gates to control which flipflops are read and written. That is far simpler than an adder.'],\n",
       "             'https://www.researchgate.net/publication/307471488_Philosophical_Principles_of_Computer_Architecture_Design_in_the_I_Ching': ['If you are on a personal connection, like at home, you can run an anti-virus scan on your device to make sure it is not infected with malware.'],\n",
       "             'https://www.uniassignment.com/essay-samples/information-technology/the-background-of-computer-architecture-information-technology-essay.php': ['Today, it is widely recognized computer is really essential for the entire world. During the 21th century, many people were realize using computer could help them successful a lot of things with easier, either that’s for calculation or management. So, as we know computer is a very useful digital machine, but not everyone actually know how it created by. Therefore in this assignment, we would discuss all the things about computer architecture.',\n",
       "              'In this assignment, it has been recognized in four sections, which are introduction, content conclusion and references, each section would go in to details. First of all, in first question we would talk about some several generations of computer central processing unit (CPU), which included the stage of design and development of early CPU as well. During this section, we will make some comparison with the latest CPU and how much faster is the design and development of the latest CPU as opposed to the beginning of the CPU inventions. After that, during the secondary question in this assignment, we would create a diagram and discuss about the bus system. In this part, we should explain more detail about the bus system in term of interconnection, transmission and architecture as well.',\n",
       "              'As a conclusion, this assignment is about the function and structure of computer. The purpose of this assignment is to present as clearly and completely as possible, the characteristics and nature of modern-day computer system. Although most of the resources of this assignment are taken from internet and reference book, the objective is to present the material in a fashion that keeps new material in a clear context to those readers.',\n",
       "              'The meaning of computer architecture can be properly defined as a specification detailing that how a set of the hardware and software technologies standards interacting to form the platform or computer system. It is refers to how compatible with a computer system technologies and its design. Likewise, computer architecture also could refer to those attribute within the system, and those attribute have a direct impact on the logical execution of a program. For example, architecture attribute have include a lot of instruction set, those are the number of bits inside the program were used to represent a various data type, and the data type can be a numbers or characters.',\n",
       "              'Besides that, I/O mechanisms and some techniques for the addressing memory also have been included within the architecture attribute. In addition, there have three type of computer architecture in our daily use, which is system design, instruction set architecture (ISA) and computer organization (known as mircoarchitecture). In short, computer architecture mostly likes to determine what the user, system or technology were needs and create some logical design and standard based on those requirement.',\n",
       "              'The history of computer development was often referring to several different generations of computing devices. Each generation of computer is characterized by its technological development. Those purposes are wanted to increate smaller, cheaper, efficient, powerful and reliable devices.',\n",
       "              'For the first generation computer, its circuitry and magnetic drums of memory almost are making by vacuum tubes. It often larger and should have a rooms size to place it. It is very expensive to operate, since this kind of computers are enormous and should use a great deal of electricity to make it work. Besides that, it also would generate a lot of heat, so that is the most common cause to become malfunction.',\n",
       "              'The first generation computers were relied on the lowest-level programming language or machine language to perform an operation system, but it just only can solve the problem at a time. By using an input, it was based on the punched cards or paper tape, and the output would be displayed on printouts. The ENIAC and UNIVAC computers are the great examples of first-generation computing devices. The UNIVAC was the first commercial computer that delivered to a business client, which is the U.S. Census Bureau in 1951.',\n",
       "              'While completely develop a second generation computing devices, the transistors had been developed and replaced vacuum tubes. In 1947, the transistor was already invented, but it is not widespread to use until 1950s. Though this transistor was far superior to vacuum tube, it became more reliable than their first generation predecessors, and allows computer to increasingly smaller, cheaper, faster, and more energy efficient. In fact, although the problem about generated a great deal of heat are haven’t solve yet, its improvement are still biggest than vacuum tube. Because of this reason, its input and output must still reliable on punched card and printout.',\n",
       "              'In addition, the cryptic binary machine language of second generation computer was evolving a change to languages, symbolic or assembly, which could allow programmer to specify a proper instruction in word form. Moreover, the high-level programming language had also being developed at the same time, such as the early version of FORTRAN and COBOL.',\n",
       "              'During this invention, the technology of magnetic drum had been changed to magnetic core, which means the first computer can store their instruction to their memory. So there have many atomic energy industry in this generation would started to use this type of computer to operate their system.',\n",
       "              'For the third generation computer, the development of integrated circuit was began the hallmark. In this generation, its transistor was evolved to become miniaturized, and it could place on the silicon chips, which is called semiconductor. Therefore, it has a decisive prerequisite to increase the efficiency and speed of the computer. Besides that, it was also instead of printouts and punched cards. The user interact with the third generation computer was through the monitors, keyboard and interface with an operating system, which would allow any device to run many kind of application at a same time, and its application should run with the central program that had been monitored within the memory. Lastly, it was increasingly smaller and cheaper than before generation.',\n",
       "              'The microprocessor was the fourth generation of the computer, it have a thousand of integrated circuits were built onto the single silicon chip. Different with the first generation, the shape of this computer now could fit into the palm of the hand as well.',\n",
       "              'For example in 1971, the Intel 4004 chip has been developed, it was located the entire component within the computer, which is from the central processing unit and memory until to the input/output controls that onto the single chip. After that, the IBM was introduced its first computer that suggest for the home user in 1981, and in 1984 Apple company introduced Macintosh. Besides that, during this generation, as a small computer to become more powerful and efficiently, Microprocessors are not only could be used in realm of desktop computer, many products in our daily use was begin to use microprocessor, for example like handheld devices, though the development of GUI, it could be easily link together and form a network, and it was led to the development of the internet.',\n",
       "              'For the concept of fifth generation computing devices, there are almost based on the artificial intelligence. Although this is still in development, there have some application like voice recognition was being to use. In addition, based on this development, the usage of parallel processing and superconductor was helping to make artificial intelligence to become more reality. There have some technologies were most radically change the face of computer, which is molecular, quantum computation and nanotechnology. Normally, the goal of fifth generation computing is to develop some device that could properly respond to the natural language input or become more capable of self-organization and learning.',\n",
       "              'The meaning of bus could properly define as a communication pathway that used to connect to two or more devices within the computer system, it also knows as a medium of sharing transmission. Whenever the multiple devices are connecting to the bus, its signal will be transmitted by any devices which their reception was available, and though receptions mostly were coming from the other devices that have been attached to the bus. In addition, if there have two devices try to transmit during at the same time period, its signal will become overlap and sometimes begin garbled. Therefore, the transmission could be successful at a time, but only did by one device.'],\n",
       "             'https://www.sigarch.org/a-brief-and-biased-history-of-computer-architecture-part-1/': ['I have a bigger agenda. My advisor, Michael D. Smith, taught me “There are no new ideas in computer architecture, only old ones whose time has come.” Mike’s maxim matches my experience: Anton used every parallelism technique I know save threading to show that special-purpose machines could deliver multiple orders of magnitude speedup; the first TPU combined systolic arrays, decoupled access/execute, and CISC, breathing new life into old (and let’s be honest, unpopular) ideas. I’ll bet that the next transformative machine also combines old ideas in surprising new ways.',\n",
       "              'As this is a blog post, I can blithely ignore peer-reviewed rigor, adopt a more informal, subjective style, fail to remember important contributions, and make tart and most likely wrong comments along the way. But I have to keep things brief, so I’m going to present my history in bullet points and summarize huge, complicated areas with sentence fragments. Also, I’m going to focus on the machines and the people who built them more than particular techniques (although those also play a key role). With all these caveats, here’s my subjective, biased, and incomplete history of computer architecture, as a first draft for the lore and epic tales of our field.',\n",
       "              'Around WWII, a number of proto-computers are built and used for cryptography, artillery ballistics tables, and the design of nuclear weapons. 5 I find the principled stances of Computer Scientists who refuse government or military funding to be admirable, but I wonder if they know how martial the origins of our field are. They include:',\n",
       "              'In 1949, the Eckert-Mauchly computer company builds the BINAC, the first commercial computer. UNIVAC follows in 1951, with the US Census as a customer. The computer industry blooms during the 50s, but the Eckert-Mauchly computer company isn’t one of the big winners.',\n",
       "              'Let me pause here, ending the first installment of this history with IBM largely in control, but with threats to their hegemony on the way. I’ll resume in Part 2 in a few days.',\n",
       "              'About the Author: Cliff Young is a software engineer in Google Research, where he works on codesign for deep learning accelerators. He is one of the designers of Google’s Tensor Processing Unit (TPU) and one of the founders of the MLPerf benchmark. Previously, Cliff built special-purpose supercomputers for molecular dynamics at D. E. Shaw Research and was a Member of Technical Staff at Bell Labs.',\n",
       "              'Disclaimer: These posts are written by individual contributors to share their thoughts on the Computer Architecture Today blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGARCH or its parent organization, ACM.',\n",
       "              'SIGARCH serves a unique community of computer professionals working on the forefront of computer design in both industry and academia. It is ACM’s primary forum to interchange ideas about tomorrow’s hardware and its interactions with software.',\n",
       "              'SIGARCH serves a unique community of computer professionals working on the forefront of computer design in both industry and academia. It is ACM’s primary forum to interchange ideas about tomorrow’s hardware and its interactions with software.',\n",
       "              'This site is maintained by volunteers working in many programs of ACM SIGARCH. We thank you for visiting! If you have questions about the site, please send a note to our content editor.']})"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14311ce",
   "metadata": {},
   "source": [
    "## Subheading classification and dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "e1a8e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:22<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "topic_subheading_dataset = defaultdict(lambda: defaultdict(list))\n",
    "for webpage, data in tqdm(raw_dataset.items()):\n",
    "    data_noun, data_for_model = [], []\n",
    "    for d in data:\n",
    "        output = NounExtractor(d)\n",
    "        data_noun.append(output[0])\n",
    "        data_for_model.append(output[1])\n",
    "    paragraph_embedding = torch.tensor(model.encode(data_for_model))\n",
    "    labels = torch.argmax((features_tensor @ paragraph_embedding.T), axis=0)\n",
    "    i = 0\n",
    "    for paragraph, label in zip(data, labels):\n",
    "        if len(set(words_related_to_topic).intersection(set(data_noun[i]))) > 2:\n",
    "            if len(topic_subheading_dataset[important_subsections[label]]) > 5:\n",
    "                continue\n",
    "            topic_subheading_dataset[important_subsections[label]][webpage].append(paragraph)\n",
    "        i += 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "aac2c788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "philosophy :  6\n",
      "hardware :  6\n",
      "overview :  5\n",
      "background :  6\n"
     ]
    }
   ],
   "source": [
    "for subheading, websites in topic_subheading_dataset.items():\n",
    "    print(subheading, ': ', len(websites))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaa60e6",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9eb3d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                          do_lower_case=True)\n",
    "summarizer = pipeline('summarization', model=\"sshleifer/distilbart-cnn-12-6\" )\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import language_tool_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "93ad4b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_dataset = defaultdict(lambda: defaultdict(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "73276b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (763 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:02<00:00, 15.52s/it]\n"
     ]
    }
   ],
   "source": [
    "for subheading, websites in tqdm(topic_subheading_dataset.items()):\n",
    "    for url, paragraphs in (websites.items()):\n",
    "        data = ''.join(paragraphs)\n",
    "        if len(tokenizer([data])['input_ids'][0]) > 1023:\n",
    "                count = 0\n",
    "                data_nlp = nlp(data)\n",
    "                sentences = list(data_nlp.sents)\n",
    "                #print(sentences)\n",
    "                data = \"\"\n",
    "                for sentence in sentences:\n",
    "                    sentence = str(sentence)\n",
    "                    #print(type(sentence))\n",
    "                    count += (2 + len(word_tokenize(sentence)))\n",
    "                    if count < 924:\n",
    "                        data += sentence\n",
    "        summary_text = summarizer(data, max_length=len(word_tokenize(data))//2\\\n",
    "                                  , min_length = len(word_tokenize(data))//4)[0]['summary_text']\n",
    "#         tool = language_tool_python.LanguageTool('en-US') \n",
    "#         summary_text = tool.correct(summary_text)\n",
    "#         tool.close()\n",
    "        summarized_dataset[subheading][url] = summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4208693d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'philosophy': defaultdict(str,\n",
       "                         {'http://www2.latech.edu/~choi/Bens/Teaching/Csc364/index.htm': ' Topics include processor, control unit and microprogramming, computer arithmetic',\n",
       "                          'https://en.wikipedia.org/wiki/Computer_architecture': \" The first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine . Konrad Zuse described in two patent applications for his future projects that machine instructions could be stored in the same storage used for data, i.e., the stored-program concept . The term “architecture” in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959 . Later, computer users came to use the term in many less explicit ways . Computers do not understand high-level programming languages such as Java, C++, or most programming languages used. A processor only understands instructions encoded in some numerical fashion, usually as binary numbers . Modern computer performance is often described in instructions\",\n",
       "                          'https://en.wikipedia.org/wiki/Computer_memory': ' In early computer systems, programs typically specified the location to write memory and what data to put there . This location was a physical location on the actual memory',\n",
       "                          'https://en.wikipedia.org/wiki/Microarchitecture': ' In computer engineering, microarchitecture is the way a given instruction set architecture (ISA) is implemented in a particular processor . The pipelined architecture allows multiple instructions to overlap in execution, much like an assembly line . A given ISA may be implemented with different microarcharchitectures;   implementations may vary due to different goals of a given design or due to shifts in technology . The pipeline includes several different stages, such as instruction fetch, instruction decode, execute, and write back . Some architectures include other stages such as memory access .',\n",
       "                          'https://en.wikipedia.org/wiki/Hardware_architecture': \" In engineering, hardware architecture refers to identification of a system's physical components and their interrelationships . This description, often called a hardware design model, allows hardware designers to understand how their components fit into a system architecture . Clear definition of a hardware architecture allows the various engineering disciplines to work more effectively together to develop and manufacture new machines, devices and components .\",\n",
       "                          'https://www.technologyuk.net/computing/computer-hardware/architecture.shtml': \" Computer architecture deals with the logical and physical design of a computer system . The Instruction Set Architecture (ISA) defines the set of machine-code instructions that the computer's central processing unit can execute . The microarchitecture describes the design features and circuitry of the\"}),\n",
       "             'hardware': defaultdict(str,\n",
       "                         {'https://en.wikipedia.org/wiki/Computer_architecture': ' Computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems',\n",
       "                          'https://en.wikipedia.org/wiki/Word_(computer_architecture)': ' In computing, a word is a fixed-sized datum handled as a unit by the instruction set or the hardware of the processor . The number of bits or digits in a word (the word size, word width, or word length) is an important characteristic of',\n",
       "                          'https://www.educba.com/types-of-computer-architecture/': ' Complex instruction set architecture is the root of compilers because earlier compilers were not there to write programs . The best performance is obtained by using simple instruction from IS',\n",
       "                          'http://www.edwardbosworth.com/My5155Textbook_HTM/MyText5155_Ch09_V06.htm': ' The UNIBUS™ on a PDP-11, a minicomputer marketed by the Digital Equipment Corporation (DEC – now a part of Hewlett-Packard) We quote from three manuals published by DEC . The control signals used are the m MAR the micro-address of the next control word to read the m MBR this holds the last control word read from micro-memory the sequencer this computes the next value of the address for the MAR . We use ten control signals to read and write memory .',\n",
       "                          'https://online.sunderland.ac.uk/what-is-computer-architecture/': ' When a software program is being written, an algorithm is reduced to the formal instructions that a von',\n",
       "                          'https://m-cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext': ' Intel gave a new team 52 weeks to develop the new \"8086\" ISA and design and build the chip . Given the tight schedule, designing the ISA took only 10 person-weeks over three regular calendar weeks .'}),\n",
       "             'overview': defaultdict(str,\n",
       "                         {'https://en.wikipedia.org/wiki/Computer_architecture': ' Computer architecture involves instruction set architecture design, microarchitecture design, logic design, and',\n",
       "                          'https://www.educba.com/types-of-computer-architecture/': ' The memory we have a single read/write memory available for read and write instructions and data . Data and instructions are stored in a single memory within the computer system .',\n",
       "                          'https://www.tutorialspoint.com/Computer-System-Architecture': ' All calculations related to the computer system are performed by the arithmetic logic unit . It can perform operations like addition, subtraction, multiplication, division etc',\n",
       "                          'https://m-cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext': ' Few general-purpose programs have branches that can be predicted so accurately . If processor architect wants to limit wasted work to only 10% of the time, processor must predict each branch correctly 99.3% .',\n",
       "                          'https://www.uniassignment.com/essay-samples/information-technology/the-background-of-computer-architecture-information-technology-essay.php': ' The meaning of computer architecture can be properly defined as a specification detailing that how a set of the hardware and software technologies standards interacting to form the platform or computer system . Computer architecture also could refer to those attribute within the system, and those attribute have a direct impact on the logical execution'}),\n",
       "             'background': defaultdict(str,\n",
       "                         {'https://en.wikipedia.org/wiki/Word_(computer_architecture)': ' Individual bytes can be accessed on a word-oriented machine in one of two ways . Bytes can be manipulated by a combination of shift and mask operations in registers . Many word-orientated machines implement byte operations with instructions using special byte pointers in registers or memory . For example, the PDP-10 byte pointer contained the size of the byte in bits (allowing different-sized bytes to be accessed)',\n",
       "                          'https://www.educba.com/types-of-computer-architecture/': ' Microarchitecture is known as computer organizations and it is the way when instruction set architecture is a built-in processor . Central processing unit (CPU) performs one operation at a time, either fetching data or instruction in/out of the memory . Control and logic units for processing operations are within the central processing unit .',\n",
       "                          'http://www.edwardbosworth.com/My5155Textbook_HTM/MyText5155_Ch09_V06.htm': ' In each of the designs above, the goal was to reduce the number of “storage units” that required the expensive and hard-to-maintain vacuum tubes . This small number of storage units became the register file associated with the central processing unit (CPU) It was not until the MIT Whirlwind in 1952 that magnetic core',\n",
       "                          'https://online.sunderland.ac.uk/what-is-computer-architecture/': ' All computers, no matter their size, are based around a set of rules stating how software and hardware join together . This is what is known as computer architecture . In this article we’re going to delve into what computer architecture actually is .',\n",
       "                          'https://www.britannica.com/science/computer-science/Architecture-and-organization': ' Computational science applies computer simulation, scientific visualization, mathematical modeling, algorithms, data structures, networking, database design, and high-performance computing to advance the goals of various disciplines . These disciplines include biology, chemistry, fluid dynamics, archaeology, finance, sociology, and forensics .',\n",
       "                          'https://m-cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext': \" Moore's Law meant control stores could become much larger . Larger memories in turn allowed much more complicated ISAs . Consider that the control store of the VAX-\"})})"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23915119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'personal life': defaultdict(str,\n",
    "#                          {'https://www.beursschouwburg.be/en/events/we-object/we-object-with-joelle-sambi-nzeba-babs-gons/': \n",
    "#                           \" 'JOËLLE SAMBI NZEBA was born in Belgium, but spent part of her childhood in \n",
    "#                           Kinshasa before returning to Brussels where she now lives and works. Alongside \n",
    "#                           her professional activities which are carried out within the context of a feminist \n",
    "#                           movement, she is a writer. She graduated from the Université Libre de Bruxelles with a \n",
    "#                           degree in information and communication (journalism) and is the author of several \n",
    "#                           prize-winning works of fiction (Je ne sais pas rêver, 2002 and Le monde est gueule \n",
    "#                           de chèvre, 2007). Through her activism (Merhaba, Festival Massimadi Bruxelles) and \n",
    "#                           writing, Joëlle Sambi Nzeba tries to question situations of powerlessness. She gets \n",
    "#                           people talking about identity, the norm and belonging.'\",\n",
    "                          \n",
    "#                           'https://bela.be/auteur/joelle-sambi': \n",
    "#                           \" Noelle Samba is co-présidente de l'Euro Central Asian Lesbian Community. She is also a member of the Belgian Network For Black Lives. Samba dissolve de provenance et travail d’scripture, LE Congo, son history et la Belgium contemporize sent enjoins presents en filigree.\",\n",
    "                          \n",
    "#                           'https://www.laicite.be/magazine-article/joelle-sambi-nzeba-portrait-pluriel/': \n",
    "#                           ' Un true entire LES Chou de Belles à la sauce cocktail days un bar poussiéreux, parfait applique de son cousin kinds, at Mating. Un non-sens quo. C’est CA la Belgium en moi. Noelle Samba Zebra.',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2409e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c068bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'overview': defaultdict(str,\n",
    "                         {'https://online.princeton.edu/computer-architecture': ' Building on a computer organization base, this course explores techniques that go into designing a modern microprocessor. Fundamental understanding of computer architecture is key not only for students interested in hardware and processor design. This course will explore how the computer architect can utilize the increasing number of transistors available to improve the performance of a processor. Focus will be given to architectures that can exploit different forms of parallelism, whether they be implicit or explicit.',\n",
    "                          'https://www.educba.com/types-of-computer-architecture/': ' Each memory has multiple locations and each location has a unique address. We can address the contents of memory by its location irrespective of what type of data and instructions are present in the memory. Microarchitecture performs in a certain way. It reads the instruction and decodes it, will find parallel data to process the instruction. It is used in microprocessors, microcontrollers.',\n",
    "                          'https://geteducationskills.com/computer-architecture/': ' This chapter provides a first examination of the principal forms of supercomputer architecture and the underlying concepts that govern their performance. It is here, at the structural and logical levels, that parallelism of operation in its many forms and size is first presented. This chapter introduces the basic foundations of computer architecture in general and for high-performance computer systems in particular. The chapter provides an overview of all computer cores, from those few in the smallest mobile phones to potentially millions making up the world’',\n",
    "                          'https://en.wikipedia.org/wiki/Computer_architecture': ' Computer architecture is concerned with balancing the performance, efficiency, cost, and reliability of a computer system. Longer and more complex instructions take longer for the processor to decode and can be more costly to implement effectively. Memory organization defines how instructions interact with the memory, and how memory interacts with itself. Computers that control machinery usually need low interrupt latencies. Multimedia projects may need very rapid data access, while virtual machines may need fast interrupts.',\n",
    "                          'https://learn.saylor.org/course/CS301': ' In this unit, we will discuss various components of MIPS processor architecture. This unit will ask you to apply the information you learned in units 2, 3, and 4 to create a simple processor architecture. We will also discuss a technique known as pipe lining, which is used to improve processor performance. The unit will conclude with a look at some programming techniques used in the context of parallel machines.'}),\n",
    "'source': defaultdict(str,\n",
    "                         {'https://online.princeton.edu/computer-architecture': ' \"Thank you for making this excellent course available! It was very insightful, the explanations were great -- it really helped to understand a lot of the behind-the-scenes magic that I\\'ve been taking for granted in 20 years as a software engineer. Thanks a lot!\" says the author of the book \"thank you a lot\" and the course was \"very insightful\"',\n",
    "                          'https://www.educba.com/types-of-computer-architecture/': ' The name defines itself, the design will satisfy user requirements such as architecture, module, interfaces and data for a system, and it is connected to product development. Modular systems are made by standardizing hardware and software. It is the process of taking marketing information and creating product design to be manufacture. Modular design is a process of standardizing software and hardware to meet user requirements.',\n",
    "                          'https://geteducationskills.com/computer-architecture/': \" Designing a computer is about designing a machine that holds and manipulates data. This book is about how to make specialized brains. It's all about processing information, processing information. Designing computers is about making computers that hold and manipulate data, rather than computers that run software and run software. The book is published by Simon Tisdale, a British-based publisher, and is available on Amazon.com.\",\n",
    "                          'https://learn.saylor.org/course/CS301': ' We will begin this unit with an overview of digital components, identifying the building blocks of digital logic. We will build on that foundation by writing truth tables and learning about more complicated sequential digital systems with memory. This unit serves as background information for the processor design techniques we learn in later units. To receive a free Course Completion Certificate, you will need a grade of 70% or higher on this final exam.'}),\n",
    "'type': defaultdict(str,\n",
    "                         {'https://online.princeton.edu/computer-architecture': ' \"I am a VLSI Design professional working in the field of CPU/SoC architecture and Design. This course helped me to reinforce the basics and also to find more interesting topics to explore and research. The course content was very good covering the essential concepts,\" says one of the course\\'s students. The course was very successful, says the author of the book.',\n",
    "                          'https://www.educba.com/types-of-computer-architecture/': \" Data and instructions are stored in a single read/write memory within the computer system. Harvard's architecture is used when data and code is present in different memory blocks. A separate memory block is needed for data and instruction. Data can be accessed by one memory location and instruction can be. Accessed by a different. Location in different types of computer architecture in different computer architectures.\",\n",
    "                          'https://www.codecademy.com/learn/computer-architecture': \" In this course, you’ll learn about what the main physical components of a computer are, why 0 and 1 are such important numbers within computing, how instruction set architecture (ISA) establishes communication between the hardware and software components. The course will teach you computer architecture with a combination of lessons, articles, quizzes, problem sets, and projects. At the end of the course you'll be prompted to create your own CPU simulator in Python.\",\n",
    "                          'https://geteducationskills.com/computer-architecture/': ' Computer engineering is a science or a set of rules stating how brain software and hardware are joined together and interact to make a computer work. It not only determines how the brain works but also of which technologies the computer is capable. The best programs for aspiring computer architects are computer-based fields because they offer students the most hands-on experience in database design or network security.',\n",
    "                          'https://en.wikipedia.org/wiki/Computer_architecture': ' Computers do not understand high-level programming languages such as Java, C++, or most programming languages used. A processor only understands instructions encoded in some numerical fashion, usually as binary numbers. Software tools, such as compilers, translate those high level languages into instructions that the processor can understand. The ISA defines items in the computer that are available to a program.',\n",
    "                          'https://learn.saylor.org/course/CS301': ' The purpose of this course is to cultivate an understanding of modern computing technology through an in-depth study of the interface between hardware and software. The course will conclude with a look at the recent switch from sequential processing to parallel processing by looking at the parallel computing models and their programming implications. You will learn about modern computer architecture and the Von Neumann architecture, pipe lining, memory management, storage, and other input/output topics.'}),\n",
    "'history': defaultdict(str,\n",
    "                         {'https://www.educba.com/types-of-computer-architecture/': ' Computer architecture consists of rules and methods or procedures which describe the implementation, functionality of the computer systems. Architecture is built as per the user’s needs by taking care of the economic and financial constraints. The computer system has the processor, memory, I/O devices and communication channels that connect to it. It has digital signal processors that will execute small or highly audio or video algorithms, and it is reproducible.',\n",
    "                          'https://www.codecademy.com/learn/computer-architecture': ' Create a simple calculator application in Python that uses a 32-bit Instruction Set Architecture that the student designs to read and execute binary instructions. It simulates the basic CPU function in the computer hierarchy. Students can go in knowing zero, nothing, and just get a grasp on everything as you go and start building right away. I know from first-hand experience that you can go into knowing zero.',\n",
    "                          'https://geteducationskills.com/computer-architecture/': ' Computer Architecture: In computer manufacturing, computer engineering is a set of rules and methods that describe the functionality, organization, and utilization of computer systems. Computer architects are expected to see an employment growth of 6% between 2016 and 2026 as reported by the U.S. Bureau of Labor Statistics. While cloud computing has decreased the need for computer architects somewhat, they will continue to be in demand as businesses continue to increase their technology needs.',\n",
    "                          'https://en.wikipedia.org/wiki/Computer_architecture': ' In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. The first documented computer architecture was in correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine. Konrad Zuse described in two patent applications for his future projects that machine instructions could be stored in the same storage used for data, i.e., the stored-program concept.',\n",
    "                          'https://learn.saylor.org/course/CS301': ' In this unit, we will discuss some advances in technology that led to the development of modern computers. We will discuss the importance of computing power and how it motivated the switch from a single-core to a multicore processor. In previous units, you learned about how computer memory stores information, how numbers are represented in a computer memory word (typically, 32 or 64 bits) We will also discuss the designs of adders, multipliers, and dividers.'})})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
